{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPS218nrJzoEKpTtJG3kRnb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Instalación"],"metadata":{"id":"CQWWrB4ygzEG"}},{"cell_type":"code","source":["!pip install numpy scipy\n","!pip install ttml\n","\n","!pip install t3f\n","\n","import os\n","\n","# Ruta al archivo initializers.py\n","init_file_path = '/usr/local/lib/python3.11/dist-packages/t3f/initializers.py'\n","\n","# Nueva versión de random_tensor con docstring correctamente formateado\n","new_random_tensor = '''def random_tensor(shape, tt_rank=2, mean=0., stddev=1., dtype=tf.float32,\n","                  name='t3f_random_tensor'):\n","    \"\"\"Generate a random TT-tensor of the given shape with given mean and stddev.\n","\n","    Entries of the generated tensor (in the full format) will be iid and satisfy\n","    E[x_{i1i2..id}] = mean, Var[x_{i1i2..id}] = stddev^2, but the distribution is\n","    in fact not Gaussian (but is close for large tensors).\n","\n","    In the current implementation only mean 0 is supported. To get\n","    a random_tensor with specified mean but tt_rank greater by 1 you can\n","    call:\n","        x = t3f.random_tensor(shape, tt_rank, stddev=stddev)\n","        x = mean * t3f.ones_like(x) + x\n","\n","    Args:\n","        shape: array representing the shape of the future tensor.\n","        tt_rank: a number or a (d+1)-element array with the desired ranks.\n","        mean: a number, the desired mean for the distribution of entries.\n","        stddev: a number, the desired standard deviation for the distribution of\n","            entries.\n","        dtype: [tf.float32] dtype of the resulting tensor.\n","        name: string, name of the Op.\n","\n","    Returns:\n","        TensorTrain containing a TT-tensor\n","    \"\"\"\n","    shape = np.array(shape)\n","    tt_rank = np.array(tt_rank)\n","    _validate_input_parameters(is_tensor=True, shape=shape, tt_rank=tt_rank)\n","\n","    num_dims = shape.size\n","    if tt_rank.size == 1:\n","        tt_rank = tt_rank * np.ones(num_dims - 1, dtype=int)\n","        tt_rank = np.insert(tt_rank, 0, 1)\n","        tt_rank = np.append(tt_rank, 1)\n","\n","    tt_rank = tt_rank.astype(int)\n","\n","    # Empirically entries of a TT tensor with cores initialized from N(0, 1)\n","    # will have variances np.prod(tt_rank) and mean 0.\n","    # We scale each TT-core to obtain the desired stddev\n","    cr_exponent = -1.0 / (2 * num_dims)\n","    var = np.prod(tt_rank ** cr_exponent)\n","    core_stddev = stddev ** (1.0 / num_dims) * var\n","    with tf.name_scope(name):\n","        tt = tensor_with_random_cores(shape, tt_rank=tt_rank, stddev=core_stddev,\n","                                      dtype=dtype)\n","\n","    if np.abs(mean) < 1e-8:\n","        return tt\n","    else:\n","        raise NotImplementedError('non-zero mean is not supported yet')\n","'''\n","\n","# Verificar si el archivo existe y modificarlo\n","if os.path.exists(init_file_path):\n","    # Leer el contenido actual\n","    with open(init_file_path, 'r') as f:\n","        content = f.read()\n","\n","    # 1. Añadir el nuevo import después de \"from t3f import shapes\"\n","    import_line = 'from t3f import shapes'\n","    new_import = 'from numbers import Integral as integer'\n","    if new_import not in content:\n","        # Asegurarse de que el import se añada solo una vez y en la posición correcta\n","        content = content.replace(import_line, f\"{import_line}\\n{new_import}\", 1)\n","\n","    # 2. Reemplazar la función random_tensor de manera más robusta\n","    start_marker = 'def random_tensor('\n","    try:\n","        start_idx = content.index(start_marker)\n","        # Buscar el final de la función buscando la siguiente definición o el final del archivo\n","        next_func_marker = 'def '\n","        end_idx = content.find(next_func_marker, start_idx + len(start_marker))\n","        if end_idx == -1:  # Si no hay más funciones, ir al final del archivo\n","            end_idx = len(content)\n","        else:\n","            # Retroceder hasta encontrar el final real de la función (antes de la próxima definición)\n","            end_idx = content.rfind('\\n\\n', start_idx, end_idx)\n","            if end_idx == -1:\n","                end_idx = content.rfind('\\n', start_idx, end_idx)\n","\n","        # Construir el nuevo contenido\n","        new_content = content[:start_idx] + new_random_tensor + content[end_idx:]\n","\n","        # Escribir el contenido modificado\n","        with open(init_file_path, 'w') as f:\n","            f.write(new_content)\n","\n","        print(f\"El archivo {init_file_path} ha sido modificado exitosamente!\")\n","    except ValueError as e:\n","        print(f\"Error al encontrar marcadores en el archivo: {e}\")\n","else:\n","    print(f\"El archivo {init_file_path} no existe o no es accesible.\")\n","\n","import tensorflow as tf\n","\n","# Configurar GPU al inicio, antes de cualquier operación de TensorFlow\n","physical_devices = tf.config.list_physical_devices('GPU')\n","if len(physical_devices) > 0:\n","    try:\n","        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","        print(\"GPU detectada y configurada para TensorFlow\")\n","    except RuntimeError as e:\n","        print(f\"Advertencia: No se pudo configurar el crecimiento de memoria en la GPU: {e}\")\n","else:\n","    print(\"No se detectó GPU, ejecutando en CPU\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5gOjwvQgyVW","executionInfo":{"status":"ok","timestamp":1749553507637,"user_tz":-120,"elapsed":29264,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}},"outputId":"95f8df67-de99-418d-cf6e-6d70809b6a5a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Collecting ttml\n","  Downloading ttml-1.0-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ttml) (2.0.2)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from ttml) (3.4.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ttml) (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from ttml) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from ttml) (1.15.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ttml) (4.67.1)\n","Collecting autoray (from ttml)\n","  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (from ttml) (2.1.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ttml) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ttml) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ttml) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->ttml) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->ttml) (3.6.0)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost->ttml) (2.21.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ttml) (1.17.0)\n","Downloading ttml-1.0-py3-none-any.whl (97 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: autoray, ttml\n","Successfully installed autoray-0.7.1 ttml-1.0\n","Collecting t3f\n","  Downloading t3f-1.2.0.tar.gz (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from t3f) (2.0.2)\n","Building wheels for collected packages: t3f\n","  Building wheel for t3f (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for t3f: filename=t3f-1.2.0-py3-none-any.whl size=69159 sha256=0443df03230856dc7107013356cda3808a5e9653c2483b7531624b592ee98488\n","  Stored in directory: /root/.cache/pip/wheels/36/ce/e5/723e43d13ef4de2e16b41e4f1aca48d999fc1528b3538277f5\n","Successfully built t3f\n","Installing collected packages: t3f\n","Successfully installed t3f-1.2.0\n","El archivo /usr/local/lib/python3.11/dist-packages/t3f/initializers.py ha sido modificado exitosamente!\n","GPU detectada y configurada para TensorFlow\n"]}]},{"cell_type":"markdown","source":["# Funciones Auxiliares"],"metadata":{"id":"mLkdi9_Dgx73"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"T6QjZWztgq6C","executionInfo":{"status":"ok","timestamp":1749553605589,"user_tz":-120,"elapsed":120,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}}},"outputs":[],"source":["import t3f\n","from ttml.tensor_train import TensorTrain\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","\n","# --- Helper function for error calculation ---\n","def calculate_relative_l2_error(y_true, y_pred):\n","    \"\"\"Calculates the relative L2 error.\"\"\"\n","    if not isinstance(y_true, np.ndarray): y_true = np.asarray(y_true)\n","    if not isinstance(y_pred, np.ndarray): y_pred = np.asarray(y_pred)\n","\n","    if y_true.size == 0 or y_pred.size == 0 or y_true.shape != y_pred.shape:\n","        # print(f\"Warning: Mismatch in y_true/y_pred for relative L2 error. y_true: {y_true.shape}, y_pred: {y_pred.shape}\")\n","        return np.inf\n","\n","    norm_true = np.linalg.norm(y_true)\n","    if norm_true < 1e-12: # Avoid division by zero or very small numbers\n","        return np.linalg.norm(y_pred) if np.linalg.norm(y_pred) > 1e-12 else 0.0\n","    return np.linalg.norm(y_pred - y_true) / norm_true\n","\n","def plot_loss_history(loss_hist, val_loss_hist, loss_metric_name='Relative L2 Error', title='Optimization Loss History'):\n","    \"\"\"\n","    Grafica el historial de pérdida de entrenamiento y validación en función de la iteración.\n","\n","    Args:\n","        loss_hist (list): Lista de valores de pérdida de entrenamiento por iteración.\n","        val_loss_hist (list): Lista de valores de pérdida de validación por iteración.\n","        loss_metric_name (str, optional): Nombre de la métrica de pérdida para las etiquetas del gráfico.\n","                                           Por defecto es 'Relative L2 Error'.\n","        title (str, optional): Título del gráfico. Por defecto es 'Optimization Loss History'.\n","    \"\"\"\n","\n","    # Las iteraciones comienzan desde -1 (para el estado inicial)\n","    # y luego 0, 1, 2, ... hasta len(loss_hist) - 2.\n","    # El eje X debe representar las iteraciones reales.\n","    # Si loss_hist[0] es la pérdida inicial (Iteración -1)\n","    # entonces loss_hist[1] es la pérdida de la Iteración 0, etc.\n","    iterations = np.arange(len(loss_hist)) - 1 # Ajustar para que el primer punto sea -1\n","\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.plot(iterations, loss_hist, label=f'Train Loss ({loss_metric_name})', color='blue', alpha=0.8)\n","    plt.plot(iterations, val_loss_hist, label=f'Validation Loss ({loss_metric_name})', color='red', alpha=0.8)\n","\n","    plt.title(title)\n","    plt.xlabel('Iteration')\n","    plt.ylabel(loss_metric_name)\n","    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n","    plt.yscale('log') # Es común que las pérdidas se grafiquen en escala logarítmica para ver la convergencia\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","\n","def process_tt_approximation_and_convert(\n","    best_tt_approx: TensorTrain,\n","    increase_amount: int = 0\n",") -> t3f.TensorTrain | None:\n","    \"\"\"\n","    Takes a ttml.TensorTrain approximation, optionally increases its rank,\n","    and then attempts to convert it to a t3f.TensorTrain object.\n","\n","    Args:\n","        best_tt_approx: The ttml.TensorTrain approximation object.\n","                        It is expected to be a valid ttml.TensorTrain instance.\n","        increase_amount (int, optional): The amount by which to increase the\n","                                         TT rank of the approximation. Defaults to 0 (no change).\n","\n","    Returns:\n","        t3f.TensorTrain | None: The converted t3f.TensorTrain object if conversion is\n","                                successful, otherwise None.\n","    \"\"\"\n","    if not isinstance(best_tt_approx, TensorTrain):\n","        print(\"\\nError: 'best_tt_approx' is not a valid ttml.TensorTrain object. Conversion skipped.\")\n","        return None\n","\n","    print(f\"Rango TT inicial: {best_tt_approx.tt_rank}\")\n","\n","    # Increase the rank of all connections by 'increase_amount' units\n","    if increase_amount > 0:\n","        best_tt_approx.increase_rank(increase_amount)\n","        print(f\"Rango TT después de aumentar en {increase_amount} unidades: {best_tt_approx.tt_rank}\")\n","    else:\n","        print(\"No se solicitó aumento de rango (increase_amount es 0 o menor).\")\n","\n","\n","    best_tt_approx_t3f = convertir_ttml_a_t3f(best_tt_approx)\n","\n","    if best_tt_approx_t3f is not None:\n","        print(\"\\nEl objeto best_tt_approx ha sido convertido exitosamente a t3f en la variable best_tt_approx_t3f.\")\n","    else:\n","        print(\"\\nLa conversión de best_tt_approx a t3f falló.\")\n","\n","    print(\"\\n--- Fin del proceso de Conversión ---\")\n","\n","    return best_tt_approx_t3f\n","\n","def augment_training_set(\n","    Omega: tf.Tensor | np.ndarray,\n","    A_Omega: tf.Tensor | np.ndarray,\n","    sizeOmegaExtra: int,\n","    number_nodes: int,\n","    d: int,\n","    custom_function: callable,\n","    seed: int = None # Added new argument for reproducibility\n",") -> tuple[tf.Tensor, tf.Tensor]:\n","    \"\"\"\n","    Augments an existing training set (Omega, A_Omega) with additional data\n","    if sizeOmegaExtra is greater than zero.\n","\n","    Args:\n","        Omega: Initial multi-indices of the training set. Can be tf.Tensor or np.ndarray.\n","        A_Omega: Initial true values corresponding to Omega. Can be tf.Tensor or np.ndarray.\n","        sizeOmegaExtra (int): The number of extra points to add to the training set.\n","                              If 0, no extra points are generated.\n","        number_nodes (int): The total number of nodes in each dimension, used by\n","                            make_omega_set and map_to_chebyshev_nodes.\n","        d (int): The dimensionality of the multi-indices.\n","        custom_function (callable): A function that takes Chebyshev nodes (np.ndarray)\n","                                    and returns the corresponding true values (np.ndarray).\n","        seed (int, optional): A seed for the random number generator to ensure reproducibility\n","                              when generating extra indices. If None, a random seed is used.\n","\n","    Returns:\n","        tuple[tf.Tensor, tf.Tensor]: A tuple containing the augmented training indices (Omega)\n","                                     and augmented training values (A_Omega), both as\n","                                     TensorFlow tensors with appropriate dtypes.\n","\n","    Raises:\n","        ValueError: If custom_function returns extra_evaluations that are not 1D or\n","                    a 2D array with a single column, or if A_Omega is not 1D or\n","                    a 2D array with a single column.\n","    \"\"\"\n","\n","    # Create a RandomState object for reproducibility\n","    rng = np.random.RandomState(seed)\n","\n","    # Convert initial Omega and A_Omega to NumPy arrays if they are TensorFlow tensors\n","    if tf.is_tensor(Omega):\n","        Omega_np = Omega.numpy()\n","    else:\n","        Omega_np = Omega\n","\n","    if tf.is_tensor(A_Omega):\n","        A_Omega_np = A_Omega.numpy()\n","    else:\n","        A_Omega_np = A_Omega\n","\n","    # Ensure A_Omega_np is 1D and of type float64\n","    if A_Omega_np.ndim > 1:\n","        if A_Omega_np.shape[1] == 1:\n","            A_Omega_np = A_Omega_np.flatten().astype(np.float64)\n","        else:\n","            raise ValueError(\"A_Omega (initial evaluations) must be 1D or a 2D array with one column.\")\n","    else:\n","        A_Omega_np = A_Omega_np.astype(np.float64) # Ensure dtype=float64\n","\n","    extra_indices = np.empty((0, d), dtype=int)\n","    extra_evaluations = np.empty((0,), dtype=np.float64)\n","\n","    # Generate extra training data if sizeOmegaExtra > 0\n","    if sizeOmegaExtra > 0:\n","        print(f\"Attempting to add {sizeOmegaExtra} extra points to the training set...\")\n","        # Step 1: Create extra set of multi-indices, now passing the random_state\n","        extra_indices = make_omega_set(number_nodes, sizeOmegaExtra, d, random_state=rng)\n","\n","        # Step 2: Map to Chebyshev nodes\n","        extra_nodes = map_to_chebyshev_nodes(extra_indices, number_nodes)\n","\n","        # Step 3: Evaluate custom_function on those nodes\n","        extra_evaluations = custom_function(extra_nodes)\n","\n","        # Ensure extra_evaluations is 1D and of type float64\n","        if extra_evaluations.ndim > 1:\n","            if extra_evaluations.shape[1] == 1:\n","                extra_evaluations = extra_evaluations.flatten().astype(np.float64)\n","            else:\n","                raise ValueError(\"custom_function must return a 1D array or a 2D array with one column for extra_evaluations.\")\n","        else:\n","            extra_evaluations = extra_evaluations.astype(np.float64) # Ensure dtype=float64\n","        print(f\"Successfully generated {len(extra_indices)} extra points.\")\n","    else:\n","        print(\"sizeOmegaExtra is 0, no extra points will be added to the training set.\")\n","\n","    # Step 4: Concatenate indices\n","    # Ensure Omega_np is 2D, even if it's empty, for vstack\n","    if Omega_np.ndim == 1:\n","        # Assuming d is the correct dimension if Omega_np is 1D and needs reshaping\n","        # This might need a more robust check based on actual data\n","        Omega_np = Omega_np.reshape(-1, d)\n","\n","    Omega_indices_augmented_np = np.vstack((Omega_np, extra_indices))\n","\n","    # Step 5: Concatenate evaluations (maintain 1D)\n","    A_Omega_augmented_np = np.hstack((A_Omega_np, extra_evaluations))\n","\n","    # Convert to TensorFlow tensors with specified dtypes\n","    Omega_tf = tf.constant(Omega_indices_augmented_np, dtype=tf.int32)\n","    A_Omega_tf = tf.constant(A_Omega_augmented_np, dtype=tf.float64)\n","\n","    print(f\"Training set augmented. New size of Omega: {len(Omega_tf)}.\")\n","\n","    return Omega_tf, A_Omega_tf\n","\n","def make_omega_set_sobol(n_nodes, size, d, seed=None):\n","    \"\"\"\n","    Genera un conjunto de multiíndices cuasialeatorios con d componentes,\n","    cada uno entre 0 y n_nodes - 1, usando Sobol sequences.\n","\n","    Parameters:\n","    - n_nodes (int): Límite superior para cada índice (rango es 0 a n_nodes - 1).\n","    - size (int): Número de multiíndices a generar.\n","    - d (int): Número de dimensiones.\n","    - seed (Optional[int]): Semilla para el motor Sobol para reproducibilidad.\n","\n","    Returns:\n","    - np.array: Array 2D de forma (size, d) con multiíndices cuasialeatorios.\n","    \"\"\"\n","    if not isinstance(n_nodes, int) or n_nodes < 1:\n","        raise ValueError(\"n_nodes must be a positive integer\")\n","    if not isinstance(size, int) or size < 0:\n","        raise ValueError(\"size must be a non-negative integer\")\n","    if not isinstance(d, int):\n","        raise ValueError(\"d must be an integer\")\n","\n","    if not (1 <= d <= 1111):\n","        raise ValueError(f\"SobolEngine dimension d must be between 1 and 1111, inclusive. Got {d}\")\n","\n","    if size == 0:\n","        return np.empty((0, d), dtype=int)\n","\n","    # Esta comprobación estaba en la función original.\n","    # np.random.randint permitiría size > total_combinations (produciendo duplicados).\n","    # Sobol discretizado también producirá duplicados si size > total_combinations.\n","    # Mantenemos la comprobación para que el comportamiento de error sea idéntico.\n","    total_combinations = n_nodes ** d\n","    if size > total_combinations:\n","        raise ValueError(f\"Requested size ({size}) exceeds total possible combinations ({total_combinations})\")\n","\n","    # Inicializar SobolEngine\n","    engine = torch.quasirandom.SobolEngine(dimension=d, scramble=True, seed=seed)\n","\n","    # Generar 'size' muestras en [0, 1)^d\n","    samples_unit_cube = engine.draw(n=size)\n","\n","    # Escalar a [0, n_nodes)^d y discretizar tomando el suelo.\n","    # Usamos float(n_nodes) para asegurar la multiplicación flotante.\n","    omega_torch = torch.floor(samples_unit_cube * float(n_nodes))\n","\n","    # Los valores de Sobol están en [0,1), así que sample * n_nodes está en [0, n_nodes).\n","    # torch.floor mapea esto a enteros {0, 1, ..., n_nodes - 1}.\n","    # No es necesario clamping adicional si n_nodes es >= 1.\n","\n","    # Convertir a array NumPy de enteros\n","    Omega = omega_torch.long().numpy()\n","\n","    return Omega\n","\n","def generate_disjoint_omega_c_sobol(Omega, sizeOmega_C, number_nodes, seed=None):\n","    \"\"\"\n","    Genera un conjunto Omega_C disjunto que no se solapa con Omega,\n","    usando Sobol sequences.\n","\n","    Parameters:\n","    - Omega (np.ndarray or tf.Tensor): Índices existentes (conjunto de entrenamiento).\n","    - sizeOmega_C (int): Tamaño del nuevo conjunto Omega_C.\n","    - number_nodes (int): Número de nodos por dimensión.\n","    - seed (Optional[int]): Semilla para el motor Sobol para reproducibilidad.\n","\n","    Returns:\n","    - np.array: Nuevo conjunto Omega_C de forma (sizeOmega_C, d).\n","    \"\"\"\n","    if TF_AVAILABLE and isinstance(Omega, tf.Tensor):\n","        Omega_np = Omega.numpy()\n","    elif isinstance(Omega, np.ndarray):\n","        Omega_np = Omega\n","    else:\n","        raise TypeError(\"Omega must be a NumPy array or TensorFlow tensor.\")\n","\n","    if not isinstance(sizeOmega_C, int) or sizeOmega_C < 0:\n","        raise ValueError(\"sizeOmega_C must be a non-negative integer\")\n","    if not isinstance(number_nodes, int) or number_nodes < 1:\n","        raise ValueError(\"number_nodes must be a positive integer\")\n","\n","    # Determinar la dimensionalidad d desde Omega.\n","    # Si Omega está vacío pero tiene forma (0, d_val), d_val se usa.\n","    # Si Omega es (N,0), d=0, lo cual dará error en la validación de Sobol.\n","    if Omega_np.ndim != 2:\n","        # Si Omega es, por ejemplo, un array vacío [], Omega_np.shape puede ser (0,).\n","        # O si sizeOmega_C es 0 y Omega es [], d podría no estar bien definido.\n","        # Si Omega está vacío y sizeOmega_C > 0, necesitamos una 'd' explícita o inferirla.\n","        # Asumimos que si Omega está vacío (shape (0,)), necesitamos que sizeOmega_C sea 0\n","        # o que d se provea de otra forma. Aquí, d se infiere de Omega.shape[1].\n","        # Si Omega = np.array([]), .shape es (0,). Esto llevaría a d=0 si se toma shape[1] directamente.\n","        # Para ser robustos, si Omega_np.shape es (0,), y sizeOmega_C > 0, no podemos inferir d.\n","        if Omega_np.shape == (0,) and sizeOmega_C > 0:\n","             raise ValueError(\"Cannot infer dimension d from an empty Omega with shape (0,).\")\n","        elif Omega_np.shape == (0,) and sizeOmega_C == 0: # Si Omega es [] y sizeOmega_C es 0\n","            # No se puede inferir d, pero no se necesitan puntos. Devolvemos (0,0) o (0,1) arbitrariamente?\n","            # Para ser consistentes, necesitamos una 'd'. Podríamos requerir d como parámetro o\n","            # decidir que un Omega (0,) no es un input válido si sizeOmega_C > 0.\n","            # La función original haría Omega.shape[1], lo que fallaría para (0,).\n","            # Asumimos que Omega siempre será 2D, e.g. np.empty((0, d_actual)).\n","            raise ValueError(\"Omega has an unexpected shape. Expected 2D array.\")\n","\n","    d = Omega_np.shape[1]\n","\n","    if not (1 <= d <= 1111):\n","        # Manejar el caso d=0 que Sobol no soporta:\n","        # Si d=0 y sizeOmega_C=0, devolver np.empty((0,0)) es razonable.\n","        # Si d=0 y sizeOmega_C=1 y Omega no contiene '()', se podría devolver np.array([[]]).\n","        # Para simplificar y adherirse al rango de Sobol:\n","        if d == 0 and sizeOmega_C == 0: # Caso especial si la forma de Omega era (N,0)\n","            return np.empty((0,0), dtype=int)\n","        raise ValueError(f\"SobolEngine dimension d must be between 1 and 1111, inclusive. Got {d} from Omega.shape[1]\")\n","\n","    if sizeOmega_C == 0:\n","        return np.empty((0, d), dtype=int)\n","\n","    total_points = number_nodes ** d\n","\n","    # Convertir Omega a un conjunto de tuplas para búsquedas rápidas\n","    all_indices = set(tuple(idx) for idx in Omega_np)\n","\n","    # Comprobar si es posible generar la cantidad solicitada\n","    if len(all_indices) + sizeOmega_C > total_points:\n","        raise ValueError(f\"Cannot generate {sizeOmega_C} unique indices. \"\n","                         f\"Available points: {total_points - len(all_indices)}.\"\n","                         \" Requested: {sizeOmega_C}.\")\n","\n","    # Límite de intentos como en la función original\n","    max_attempts = 10 * sizeOmega_C # O un valor más grande si se espera alta tasa de colisión\n","\n","    Omega_C_list = []\n","    attempts = 0\n","\n","    engine = torch.quasirandom.SobolEngine(dimension=d, scramble=True, seed=seed)\n","\n","    while len(Omega_C_list) < sizeOmega_C and attempts < max_attempts:\n","        # Generar un candidato usando Sobol\n","        # sample_unit_cube tiene forma (1, d)\n","        sample_unit_cube = engine.draw(n=1)\n","\n","        candidate_torch = torch.floor(sample_unit_cube * float(number_nodes))\n","        # Asegurar que esté en el rango [0, number_nodes-1]\n","        # No es estrictamente necesario el clamp con Sobol [0,1) y floor.\n","        # candidate_torch = torch.clamp(candidate_torch, min=0, max=number_nodes - 1)\n","\n","        candidate = candidate_torch.long().numpy()[0] # Extraer el array 1D de la forma (1,d)\n","\n","        if tuple(candidate) not in all_indices:\n","            Omega_C_list.append(candidate)\n","            all_indices.add(tuple(candidate))\n","\n","        attempts += 1\n","\n","    if len(Omega_C_list) < sizeOmega_C:\n","        raise ValueError(\"Could not generate enough unique indices for Omega_C within max_attempts.\")\n","\n","    # Convertir lista de arrays a un array 2D NumPy\n","    # Si Omega_C_list está vacía (aunque sizeOmega_C > 0 y falló), np.array([]) da forma (0,).\n","    # Pero ya tenemos el chequeo de sizeOmega_C == 0 arriba.\n","    # Si len(Omega_C_list) < sizeOmega_C, se lanza error.\n","    # Así que si llegamos aquí, len(Omega_C_list) == sizeOmega_C > 0.\n","    return np.array(Omega_C_list, dtype=int)\n","\n","def format_value_dynamically(value, fixed_digits=4, scientific_digits=4):\n","    \"\"\"\n","    Formatea un valor numérico.\n","    Usa notación científica si el valor es muy pequeño (pero no cero)\n","    y se redondearía a cero con la notación de punto fijo especificada.\n","    De lo contrario, usa notación de punto fijo.\n","\n","    Args:\n","        value (float): El número a formatear.\n","        fixed_digits (int): Número de decimales para la notación de punto fijo.\n","        scientific_digits (int): Número de decimales para la mantisa en notación científica.\n","                                 (ej., .4e produce X.YYYYeNN)\n","    Returns:\n","        str: El número formateado como cadena.\n","    \"\"\"\n","    # El umbral para cambiar a notación científica.\n","    # Si abs(value) es menor que 0.5 * 10^(-fixed_digits), entonces :.Nf lo redondeará a 0.0...0.\n","    # Por ejemplo, para fixed_digits=4 (0.0001), el umbral es 0.00005.\n","    threshold = 0.5 * (10**-fixed_digits)\n","\n","    if 0 < abs(value) < threshold:\n","        return f\"{value:.{scientific_digits}e}\"  # Notación científica, ej: \"1.2345e-08\"\n","    else:\n","        return f\"{value:.{fixed_digits}f}\"     # Notación de punto fijo, ej: \"0.0001\"\n","\n","# Modified physical_point_index_fun to COLLECT indices and values\n","# Collection happens HERE.\n","def physical_point_index_fun(indices_array): # Argument points_1d removed.\n","    \"\"\"\n","    Converts index arrays (0 to MODAL_SIZE-1) to physical points (Chebyshev nodes in [0,1])\n","    using map_to_chebyshev_nodes.\n","    CALLS custom_function with physical points.\n","    COLLECTS the input index/output value pairs for this batch.\n","    Uses the global MODAL_SIZE and collects into global lists.\n","    \"\"\"\n","    # Use global lists for collection\n","    global collected_indices, collected_evaluations\n","    global d # For dimension check\n","    global MODAL_SIZE # For mapping\n","\n","    # --- Robust Input Handling ---\n","    # Ensure indices_input is a 2D array (num_samples, d)\n","    indices_input_processed = None\n","    num_samples_in_batch = 0\n","\n","    if isinstance(indices_array, np.ndarray):\n","        if indices_array.ndim == 1:\n","            indices_input_processed = indices_array.reshape(1, -1)\n","            num_samples_in_batch = 1\n","        elif indices_array.ndim == 2:\n","            indices_input_processed = indices_array\n","            num_samples_in_batch = indices_array.shape[0]\n","        elif indices_array.ndim == 3 and indices_array.shape[1] == 1:\n","            # Flatten (num_samples, 1, d) to (num_samples, d)\n","            indices_input_processed = indices_array[:, 0, :]\n","            num_samples_in_batch = indices_array.shape[0]\n","        # Add checks for other potential shapes if observed\n","        else:\n","            # Attempt to reshape to 2D (num_samples, d) based on total elements\n","            total_elements = indices_array.size\n","            if total_elements > 0 and d > 0 and total_elements % d == 0:\n","                try:\n","                    temp_reshaped = indices_array.reshape(-1, d)\n","                    indices_input_processed = temp_reshaped\n","                    num_samples_in_batch = indices_input_processed.shape[0]\n","                    # print(f\"Debug (physical_point): Reshaped unexpected shape {indices_array.shape} to {indices_input_processed.shape}\")\n","                except Exception as e:\n","                     print(f\"Error reshaping indices (unexpected shape {indices_array.shape}) in physical_point_index_fun: {e}. Skipping batch processing and collection.\")\n","                     # Cannot process indices correctly. Return empty array and no collection.\n","                     return np.array([]) # Return empty 1D array.\n","\n","            else:\n","                 print(f\"Error: Cannot process indices with unexpected shape {indices_array.shape} and size {total_elements} in physical_point_index_fun. Skipping batch processing and collection.\")\n","                 return np.array([]) # Return empty 1D array\n","\n","\n","    else:\n","         print(f\"Error: Unexpected indices type: {type(indices_array)} in physical_point_index_fun. Skipping batch processing and collection.\")\n","         return np.array([]) # Return empty 1D array\n","\n","\n","    if num_samples_in_batch <= 0:\n","         # print(f\"Debug: physical_point_index_fun received batch with 0 samples.\")\n","         return np.array([]) # Return empty 1D array\n","\n","\n","    # --- Mapping to Physical Points ---\n","    # Use map_to_chebyshev_nodes with the number of nodes per dimension (MODAL_SIZE)\n","    # Assumes indices_input_processed is (num_samples, d)\n","    try:\n","        physical_points = map_to_chebyshev_nodes(indices_input_processed, MODAL_SIZE)\n","    except ValueError as e:\n","         print(f\"Error during map_to_chebyshev_nodes in physical_point_index_fun: {e}. Skipping batch processing and collection.\")\n","         return np.array([]) # Return empty 1D array\n","    except Exception as e:\n","         print(f\"Unexpected error during map_to_chebyshev_nodes in physical_point_index_fun: {e}. Skipping batch processing and collection.\")\n","         return np.array([]) # Return empty 1D array\n","\n","\n","    # --- Call Actual Function ---\n","    # Call custom_function with the physical points. This increments total_vector_evaluations.\n","    # custom_function expects (num_samples, d) and returns (num_samples,)\n","    try:\n","        values = custom_function(physical_points)\n","        # Ensure values is a numpy array of shape (num_samples,)\n","        values = np.asarray(values) # Ensure it's a numpy array first\n","        if values.ndim > 1: # Flatten if it's not 1D, e.g., (num_samples, 1)\n","             values = values.flatten()\n","\n","        if len(values) != num_samples_in_batch:\n","             # This indicates custom_function returned a non-matching number of values.\n","             # This is a critical issue for pairing.\n","             print(f\"CRITICAL Mismatch: custom_function returned {len(values)} values for {num_samples_in_batch} input points. Expected {num_samples_in_batch}. Skipping batch collection for this batch.\")\n","             # We cannot reliably pair indices and values. Return empty array.\n","             return np.array([])\n","\n","    except Exception as e:\n","         print(f\"Error during custom_function call in physical_point_index_fun: {e}. Skipping batch collection for this batch.\")\n","         # If custom_function fails, we can't get values. Skip collection.\n","         return np.array([]) # Return empty 1D array\n","\n","\n","    # --- Collection happens HERE ---\n","    # We have indices_input_processed ((num_samples, d)) and values ((num_samples,))\n","    # Both should have length num_samples_in_batch at this point if no errors occurred above.\n","\n","    # Convert indices to list of lists of ints\n","    indices_to_collect_list = [list(row.astype(int)) for row in indices_input_processed]\n","\n","    # Convert values to list of scalars\n","    values_to_collect_list = values.tolist() # 'values' is already a 1D numpy array here\n","\n","    # Final check for consistency before extending global lists (should pass if no errors above)\n","    if len(indices_to_collect_list) != len(values_to_collect_list):\n","         # This indicates a logic error before this point, or a very strange custom_function output\n","         print(f\"CRITICAL Logic Error: Mismatch before global collection. Indices list length {len(indices_to_collect_list)}, Values list length {len(values_to_collect_list)}. Skipping batch collection.\")\n","         # Do not extend global lists if this happens.\n","    else:\n","        # Normal case: counts match within the batch. Extend global lists.\n","        collected_indices.extend(indices_to_collect_list)\n","        collected_evaluations.extend(values_to_collect_list)\n","        # print(f\"Debug: Successfully collected {len(indices_to_collect_list)} pairs from batch in physical_point_index_fun.\")\n","\n","\n","    # Return the values array from custom_function to the calling function chain\n","    # The caller expects an array matching the number of input indices.\n","    return values\n","\n","\n","# Modificada create_tt_initial y get_tt_shape - ya se hicieron en pasos anteriores\n","\n","# Helper function to get the shape of a ttml.TensorTrain\n","def get_tt_shape(tt):\n","    \"\"\"\n","    Compute the shape of a ttml.TensorTrain by inspecting the dimensions of its cores.\n","    \"\"\"\n","    shape = []\n","    if tt is None or not hasattr(tt, 'cores') or not isinstance(tt.cores, list):\n","        return None\n","\n","    if not tt.cores:\n","        return tuple()\n","\n","    for i, core in enumerate(tt.cores):\n","        if not isinstance(core, np.ndarray) or core.ndim != 3:\n","             print(f\"Warning: Core {i} is not a 3D numpy array. Shape cannot be determined.\")\n","             return None\n","        n_k = core.shape[1]\n","        shape.append(n_k)\n","\n","    global d # Use global d\n","    if len(shape) != d:\n","        print(f\"Warning: Inferred shape dimension ({len(shape)}) does not match global dimension d ({d}).\")\n","\n","    return tuple(shape)\n","\n","def make_omega_set(n_nodes, size, d, random_state: np.random.RandomState):\n","    \"\"\"\n","    Generate a set of random multi-indices with d components, each between 0 and n_nodes - 1.\n","\n","    Parameters:\n","    - n_nodes (int): Upper bound for each index (range is 0 to n_nodes - 1).\n","    - size (int): Number of multi-indices to generate.\n","    - d (int): Number of dimensions.\n","    - random_state (np.random.RandomState): The random state object to use for reproducibility.\n","\n","    Returns:\n","    - np.array: 2D array of shape (size, d) with random multi-indices.\n","    \"\"\"\n","    if n_nodes < 1:\n","        raise ValueError(\"n_nodes must be positive\")\n","    if size < 0:\n","        raise ValueError(\"size must be non-negative\")\n","\n","    total_combinations = n_nodes ** d\n","    if size > total_combinations:\n","        raise ValueError(f\"Requested size ({size}) exceeds total possible combinations ({total_combinations})\")\n","\n","    # Generate random multi-indices (0 to n_nodes - 1) using the provided random_state\n","    Omega = random_state.randint(0, n_nodes, size=(size, d))\n","\n","    return Omega\n","\n","# Additional helper function needed for adaptive sampling\n","def generate_disjoint_omega_c(Omega, sizeOmega_C, number_nodes, random_state: np.random.RandomState):\n","    \"\"\"\n","    Generate a disjoint Omega_C set that does not overlap with Omega, using a provided random_state.\n","\n","    Parameters:\n","    - Omega: Existing indices (training set), can be tf.Tensor or np.ndarray.\n","    - sizeOmega_C: Size of the new Omega_C set.\n","    - number_nodes: Number of nodes per dimension.\n","    - random_state (np.random.RandomState): The random state object to use for reproducibility.\n","\n","    Returns:\n","    - np.array: New Omega_C set of shape (sizeOmega_C, d).\n","    \"\"\"\n","    if isinstance(Omega, tf.Tensor):\n","        Omega = Omega.numpy()  # Convert to NumPy if it's a tensor\n","\n","    # Handle the case where Omega is empty\n","    if Omega.size == 0:\n","        d = Omega.shape[1] if Omega.ndim > 1 else 0 # Get dimension from shape if not empty\n","        if d == 0: # If Omega is empty and has no defined dimension (e.g., np.empty((0,0)))\n","            if sizeOmega_C > 0: # If we need to generate new points, d must be inferable\n","                raise ValueError(\"Cannot infer dimension 'd' from an empty Omega set if sizeOmega_C > 0.\")\n","            else: # If both are empty, return empty array\n","                return np.empty((0, 0), dtype=int)\n","    else:\n","        d = Omega.shape[1] # Dimension of the multi-indices\n","\n","    total_points = number_nodes ** d\n","\n","    if sizeOmega_C > total_points:\n","        raise ValueError(f\"Requested size for Omega_C ({sizeOmega_C}) exceeds total possible unique points ({total_points}).\")\n","\n","    all_indices = set(tuple(idx) for idx in Omega)\n","\n","    # Calculate how many points are available to choose from\n","    available_points_count = total_points - len(all_indices)\n","    if sizeOmega_C > available_points_count:\n","        raise ValueError(f\"Requested size for Omega_C ({sizeOmega_C}) exceeds available unique points ({available_points_count}) not in Omega.\")\n","\n","    max_attempts_per_point = 100 # Max attempts to find a unique point\n","    max_total_attempts = max_attempts_per_point * sizeOmega_C\n","\n","    Omega_C = []\n","    attempts = 0\n","    while len(Omega_C) < sizeOmega_C and attempts < max_total_attempts:\n","        # Generate candidate using the provided random_state\n","        candidate = random_state.randint(0, number_nodes, size=(d,))\n","        if tuple(candidate) not in all_indices:\n","            Omega_C.append(candidate)\n","            all_indices.add(tuple(candidate)) # Add to the set of seen indices to avoid future duplicates\n","        attempts += 1\n","\n","    if len(Omega_C) < sizeOmega_C:\n","        raise ValueError(f\"Could not generate enough unique indices for Omega_C. Requested: {sizeOmega_C}, Generated: {len(Omega_C)}. Consider increasing max_attempts_per_point or checking if enough disjoint points exist.\")\n","\n","    return np.array(Omega_C)\n","\n","# Función para generar nodos de Chebyshev en el intervalo [0, 1]\n","def chebyshev_nodes(n):\n","    \"\"\"\n","    Generate Chebyshev nodes of the first kind in the interval [0, 1].\n","\n","    Parameters:\n","    - n (int): Number of intervals (number of nodes will be n + 1). Must be >= 0.\n","\n","    Returns:\n","    - np.array: Array of (n + 1) Chebyshev nodes in [0, 1].\n","    \"\"\"\n","    if n < 0:\n","         raise ValueError(\"Number of intervals n must be non-negative\")\n","    if n == 0:\n","        return np.array([0.5]) # Convention for 1 point\n","    k = np.arange(n + 1)  # Indices 0 to n\n","    # Evitar división por cero si n es 0, aunque ya lo manejamos.\n","    # Para n>0, np.cos(np.pi * k / n) es seguro.\n","    q = np.cos(np.pi * k / n)  # Nodos en [-1, 1]\n","    x = (q + 1) / 2  # Mapeo a [0, 1]\n","    return x\n","\n","# Función para mapear índices a nodos de Chebyshev\n","def map_to_chebyshev_nodes(Omega, n_nodes):\n","    \"\"\"\n","    Map multi-indices from range [0, n_nodes - 1] to Chebyshev nodes in [0, 1] for each dimension.\n","\n","    Parameters:\n","    - Omega: 2D NumPy array of shape (size, d) with multi-indices (integers 0 to n_nodes-1).\n","    - n_nodes: Total number of nodes per dimension (size of the mode). Must be >= 1.\n","\n","    Returns:\n","    - 2D NumPy array of shape (size, d) with values mapped to Chebyshev nodes.\n","    \"\"\"\n","    if n_nodes < 1:\n","        raise ValueError(\"n_nodes (total number of points) must be positive\")\n","\n","    # chebyshev_nodes(n_nodes - 1) generates n_nodes points\n","    num_intervals_for_cheb = n_nodes - 1\n","\n","    cheb_nodes = chebyshev_nodes(num_intervals_for_cheb)\n","\n","    # Map each index to the corresponding Chebyshev node\n","    # Assumes Omega is (size, d)\n","    global d # Use the global d\n","    if Omega.ndim != 2 or Omega.shape[1] != d:\n","         # This check is better placed here for map_to_chebyshev_nodes's expected input format\n","         raise ValueError(f\"Input Omega to map_to_chebyshev_nodes must be 2D (size, d). Got shape {Omega.shape}\")\n","\n","    Omega_mapped = np.zeros_like(Omega, dtype=float)\n","    # Ensure indices are integers before using them for indexing\n","    indexed_omega = Omega.astype(int)\n","\n","    # Check index range validity (optional but good for debugging)\n","    # if np.any(indexed_omega < 0) or np.any(indexed_omega >= n_nodes):\n","    #     min_idx = np.min(indexed_omega)\n","    #     max_idx = np.max(indexed_omega)\n","    #     print(f\"Warning: Indices in Omega ({Omega.shape}) expected to be in range [0, {n_nodes - 1}] for n_nodes={n_nodes}. Found min: {min_idx}, max: {max_idx}\")\n","\n","\n","    for dim in range(d):\n","        # Look up the physical point for each index in this dimension\n","        # This might fail if indexed_omega[:, dim] contains values outside [0, num_intervals_for_cheb]\n","        # The check above should catch this.\n","        Omega_mapped[:, dim] = cheb_nodes[indexed_omega[:, dim]]\n","\n","    return Omega_mapped\n","\n","def create_tt_initial(ranks, n, d, constant_value=1.0):\n","    \"\"\"Crea un tt_initial con los rangos especificados y elementos constantes.\"\"\"\n","    cores = []\n","    for k in range(d):\n","        if k == 0:\n","            cores.append(np.ones((1, n + 1, ranks[1])) * constant_value)\n","        elif k == d - 1:\n","            cores.append(np.ones((ranks[d-1], n + 1, 1)) * constant_value)\n","        else:\n","            cores.append(np.ones((ranks[k], n + 1, ranks[k+1])) * constant_value)\n","    return TensorTrain(cores)\n","\n","def create_tt_random(ranks, n, d, seed=None):\n","    \"\"\"Crea un tt_initial con los rangos especificados y elementos aleatorios entre 0 y 1.\"\"\"\n","    cores = []\n","    rng = np.random.default_rng(seed)  # Utiliza el nuevo sistema de RNG para mayor control\n","\n","    for k in range(d):\n","        if k == 0:\n","            cores.append(rng.random((1, n + 1, ranks[1])))\n","        elif k == d - 1:\n","            cores.append(rng.random((ranks[d - 1], n + 1, 1)))\n","        else:\n","            cores.append(rng.random((ranks[k], n + 1, ranks[k + 1])))\n","    return TensorTrain(cores)\n","\n","def convertir_ttml_a_t3f(ttml_tensor):\n","    \"\"\"\n","    Convierte un objeto ttml.TensorTrain a un objeto t3f.TensorTrain.\n","\n","    Args:\n","        ttml_tensor: El objeto ttml.TensorTrain a convertir.\n","\n","    Returns:\n","        t3f.TensorTrain or None: El objeto t3f.TensorTrain convertido,\n","                                 o None si la conversión falla o la entrada es None.\n","    \"\"\"\n","    if ttml_tensor is None:\n","        print(\"Error: El objeto ttml.TensorTrain de entrada es None. No se puede convertir.\")\n","        return None\n","\n","    print(\"\\n--- Proceso de Conversión a t3f ---\")\n","\n","    # Imprimir la forma del objeto ttml original usando la función proporcionada\n","    try:\n","        ttml_shape = get_tt_shape(ttml_tensor)\n","        print(f\"Forma inferida del ttml.TensorTrain original (usando get_tt_shape): {ttml_shape}\")\n","    except Exception as e:\n","        print(f\"Error al obtener la forma del ttml.TensorTrain original (usando get_tt_shape): {e}\")\n","        ttml_shape = None # Indicar que la forma no se pudo determinar\n","\n","    # Obtener la lista de tensores core del objeto ttml.TensorTrain\n","    ttml_cores = ttml_tensor.cores\n","\n","    # Crear un objeto t3f.TensorTrain usando la misma lista de cores.\n","    # El constructor de t3f.TensorTrain infiere la forma y los rangos\n","    # a partir de las formas de los cores proporcionados.\n","    # La forma (rank_in, size, rank_out) de los cores de ttml es compatible con t3f.\n","    try:\n","        # Intentar crear el objeto t3f.TensorTrain\n","        t3f_tt = t3f.TensorTrain(ttml_cores)\n","\n","        print(\"Creación del objeto t3f.TensorTrain exitosa.\")\n","\n","        # Intentar imprimir las propiedades del objeto t3f.\n","        print(\"\\nPropiedades del t3f.TensorTrain convertido:\")\n","\n","        # Usar .get_shape() (método) para obtener la forma del tensor completo\n","        print(f\"  Shape (usando get_shape()): {t3f_tt.get_shape()}\")\n","\n","        # Mostrar los rangos TT\n","        print(f\"  TT Rank (usando t3f.tt_ranks()): {t3f.tt_ranks(t3f_tt)}\")\n","        # Puedes acceder a los cores de t3f_tt usando t3f_tt.cores\n","\n","        return t3f_tt\n","\n","    except Exception as e:\n","        # El mensaje de error modificado muestra el tipo de excepción y el mensaje original\n","        print(f\"Error inesperado al crear o acceder a atributos del objeto t3f.TensorTrain: {type(e).__name__}: {e}\")\n","        print(\"Esto podría indicar un problema con la instalación, la versión o el backend de t3f, o un problema subyacente con los cores de entrada.\")\n","        print(\"Asegúrate de que t3f está correctamente instalado ('pip install t3f') y de que los cores de ttml tienen las formas esperadas.\")\n","        return None\n","\n","# --- Funciones auxiliares ---\n","def random_idx_t3f(tt_tensor, N, random_state: np.random.RandomState):\n","    \"\"\"\n","    Generate N random indices for a t3f tensor train using a provided random_state.\n","    \"\"\"\n","    dims = tt_tensor.shape # Usamos .shape en lugar de .get_shape().as_list() para compatibilidad general.\n","    # dims es una lista de [dim0, dim1, ..., dim_d-1]\n","    idx = np.stack([random_state.choice(dim, size=N) for dim in dims], axis=-1)\n","    return idx\n","\n","# --- FUNCIÓN NUEVA: Generar el conjunto de validación (test set) ---\n","def generate_validation_set(\n","    sizeOmega_C: int,\n","    d: int,\n","    number_nodes: int,\n","    custom_function,\n","    seed: int\n","):\n","    \"\"\"\n","    Genera el conjunto de validación (Omega_C y A_Omega_C) de manera reproducible.\n","\n","    Args:\n","        sizeOmega_C (int): Número de puntos para el conjunto de validación.\n","        d (int): Dimensión del tensor.\n","        number_nodes (int): Número de nodos por dimensión.\n","        custom_function (callable): La función verdad fundamental.\n","        seed (int): La semilla para el generador de números aleatorios.\n","\n","    Returns:\n","        tuple: (Omega_C_tf, A_Omega_C_tf) como tensores de TensorFlow.\n","    \"\"\"\n","    rng = np.random.RandomState(seed)  # Generador de números aleatorios con la semilla\n","\n","    # Generar Omega_C_indices aleatoriamente sin lógica de disjuntos\n","    Omega_C_indices_np = rng.randint(0, number_nodes, size=(sizeOmega_C, d))\n","    Omega_C_mapped_np = map_to_chebyshev_nodes(Omega_C_indices_np, number_nodes)\n","    A_Omega_C_np = custom_function(Omega_C_mapped_np).astype(np.float64)\n","\n","    # Asegurarse de que A_Omega_C_np sea 1D y de tipo float64\n","    if A_Omega_C_np.ndim > 1:\n","        if A_Omega_C_np.shape[1] == 1:\n","            A_Omega_C_np = A_Omega_C_np.flatten().astype(np.float64)\n","        else:\n","            raise ValueError(\"custom_function debe devolver un array 1D o un array 2D con una columna para A_Omega_C\")\n","    else:\n","        A_Omega_C_np = A_Omega_C_np.astype(np.float64)\n","\n","    Omega_C_tf = tf.constant(Omega_C_indices_np, dtype=tf.int32)\n","    A_Omega_C_tf = tf.constant(A_Omega_C_np, dtype=tf.float64)\n","\n","    return Omega_C_tf, A_Omega_C_tf\n","\n","def process_and_verify(indices_list, evaluations_list, n_chebyshev_nodes):\n","    \"\"\"\n","    Procesa las listas de índices y evaluaciones, elimina repeticiones emparejadas\n","    y verifica la correspondencia con la evaluación de la función en los nodos de Chebyshev.\n","\n","    Args:\n","        indices_list: Lista de listas o array-like de multi-índices.\n","        evaluations_list: Lista o array-like de valores de evaluación.\n","\n","    Returns:\n","        Una tupla conteniendo:\n","            - final_indices: Array NumPy de los multi-índices únicos.\n","            - final_evals: Array NumPy de los valores de evaluación correspondientes.\n","    \"\"\"\n","    # Convertir a los tipos necesarios\n","    best_indices_list = np.array(indices_list, dtype=int)\n","    best_evaluations_list = np.array(evaluations_list, dtype=np.float64)\n","\n","    # Eliminar repeticiones emparejadas\n","    final_indices, final_evals, counts = remove_paired_repetitions(best_indices_list, best_evaluations_list)\n","\n","    if final_indices is not None:\n","        # Verificar la correspondencia con la evaluación en los nodos de Chebyshev\n","        verify_evaluation_with_mapping(final_indices, final_evals, n_chebyshev_nodes)\n","        return final_indices, final_evals\n","    else:\n","        return None, None\n","\n","def remove_paired_repetitions(indices_list, evaluations_list):\n","    \"\"\"\n","    Elimina las repeticiones de ambos arrays basándose en las repeticiones encontradas en indices_list,\n","    manteniendo la primera ocurrencia de cada elemento único.\n","    \"\"\"\n","    if len(indices_list) != len(evaluations_list):\n","        print(\"¡Las listas tienen longitudes diferentes, no se puede procesar!\")\n","        return None, None, None\n","\n","    initial_indices_count = len(indices_list)\n","    initial_evals_count = len(evaluations_list)\n","\n","    unique_indices, first_occurrence_indices = np.unique(indices_list, axis=0, return_index=True)\n","    sorted_first_occurrence_indices = np.sort(first_occurrence_indices)\n","\n","    final_indices_list = indices_list[sorted_first_occurrence_indices]\n","    final_evaluations_list = evaluations_list[sorted_first_occurrence_indices]\n","\n","    final_indices_count = len(final_indices_list)\n","    final_evals_count = len(final_evaluations_list)\n","\n","    print(\"Conteo de elementos antes de eliminar repeticiones:\")\n","    print(f\"  best_indices_list: {initial_indices_count}\")\n","    print(f\"  best_evaluations_list: {initial_evals_count}\")\n","\n","    print(\"\\nConteo de elementos después de eliminar repeticiones:\")\n","    print(f\"  best_indices_list: {final_indices_count}\")\n","    print(f\"  best_evaluations_list: {final_evals_count}\")\n","\n","    if final_indices_count == final_evals_count:\n","        print(\"\\nEl número de elementos coincide en ambas listas después de eliminar repeticiones.\")\n","    else:\n","        print(\"\\n¡El número de elementos NO coincide en ambas listas después de eliminar repeticiones!\")\n","\n","    return final_indices_list, final_evaluations_list, (initial_indices_count, initial_evals_count, final_indices_count, final_evals_count)\n","\n","def verify_evaluation_with_mapping(final_indices, final_evals, n_nodes=20): # Este valor esta puesto por defect\n","    \"\"\"\n","    Comprueba si el i-ésimo elemento de final_evals corresponde a la\n","    evaluación de custom_function en el mapeo del i-ésimo elemento de final_indices\n","    a los nodos de Chebyshev.\n","    \"\"\"\n","    if final_indices is None or final_evals is None or len(final_indices) != len(final_evals):\n","        print(\"¡Las listas finales no son válidas o tienen longitudes diferentes!\")\n","        return\n","\n","    mismatched_count = 0\n","    for i in range(len(final_indices)):\n","        mapped_vector = map_to_chebyshev_nodes(final_indices[i].reshape(1, -1), n_nodes)\n","        evaluated_value = custom_function(mapped_vector[0])\n","        expected_value = final_evals[i]\n","\n","        if not np.allclose(evaluated_value, expected_value):\n","            mismatched_count += 1\n","            print(f\"¡Desajuste en el índice {i}!\")\n","            print(f\"  Vector de índice: {final_indices[i]}\")\n","            print(f\"  Vector mapeado: {mapped_vector[0]}\")\n","            print(f\"  Valor esperado: {expected_value}\")\n","            print(f\"  Valor calculado: {evaluated_value}\")\n","\n","    if mismatched_count == 0:\n","        print(\"¡Todos los elementos de final_evals corresponden a la evaluación de custom_function en los nodos de Chebyshev mapeados desde final_indices!\")\n","    else:\n","        print(f\"\\nSe encontraron {mismatched_count} desajustes entre final_evals y la evaluación de los nodos de Chebyshev mapeados desde final_indices.\")\n","\n","# Función optimizada para submuestrear best_indices_list usando selección greedy en GPU\n","def subsample_indices(indices, subsample_size, d, initial_random=2000, batch_size=200):\n","    \"\"\"\n","    Submuestrea un conjunto de índices usando selección aleatoria inicial y greedy en GPU con lotes.\n","\n","    Parameters:\n","    - indices: np.array con los índices a submuestrear (array 2D).\n","    - subsample_size: número de índices a seleccionar.\n","    - d: número de dimensiones de los índices.\n","    - initial_random: número de índices a seleccionar aleatoriamente al inicio.\n","    - batch_size: número de índices a seleccionar por iteración greedy.\n","\n","    Returns:\n","    - np.array con el subconjunto seleccionado.\n","    \"\"\"\n","    n = len(indices)\n","    if subsample_size >= n:\n","        print(f\"Advertencia: subsample_size ({subsample_size}) es mayor o igual al número de índices ({n}). Devolviendo todos los índices.\")\n","        return indices\n","\n","    # Seleccionar un subconjunto aleatorio inicial\n","    initial_random = min(initial_random, subsample_size, n)\n","    initial_selected = np.random.choice(n, initial_random, replace=False)\n","    selected = list(initial_selected)\n","    remaining = np.setdiff1d(np.arange(n), selected)\n","\n","    indices_tf = tf.constant(indices, dtype=tf.int32)\n","    total_iterations = (subsample_size - len(selected) + batch_size - 1) // batch_size\n","    print(f\"Iniciando submuestreo: {len(selected)}/{subsample_size} índices seleccionados inicialmente. \"\n","          f\"Total de iteraciones greedy: {total_iterations}\")\n","\n","    # Añadir índices greedy por lotes\n","    for i in range(total_iterations):\n","        batch = min(batch_size, subsample_size - len(selected))\n","        if batch <= 0 or len(remaining) == 0:\n","            print(\"No hay más índices restantes o se alcanzó el tamaño deseado.\")\n","            break\n","\n","        print(f\"Iteración {i+1}/{total_iterations}: Calculando distancias para {len(remaining)} índices restantes...\")\n","        # Convertir índices a tensores\n","        remaining_tf = tf.constant(indices[remaining], dtype=tf.int32)\n","        selected_tf = tf.constant(indices[selected], dtype=tf.int32)\n","\n","        # Calcular distancias de Hamming en GPU\n","        remaining_exp = tf.expand_dims(remaining_tf, 1)  # (n_rem, 1, d)\n","        selected_exp = tf.expand_dims(selected_tf, 0)  # (1, n_sel, d)\n","        dists = tf.reduce_mean(tf.cast(remaining_exp != selected_exp, tf.float32), axis=2)  # (n_rem, n_sel)\n","        min_dists = tf.reduce_min(dists, axis=1)  # (n_rem,)\n","\n","        # Seleccionar los 'batch' índices con mayor distancia mínima\n","        _, top_indices = tf.math.top_k(min_dists, k=batch)\n","        top_indices = top_indices.numpy()\n","\n","        selected.extend(remaining[top_indices])\n","        remaining = np.delete(remaining, top_indices)\n","\n","        print(f\"Progreso: {len(selected)}/{subsample_size} índices seleccionados \"\n","              f\"({(len(selected)/subsample_size)*100:.1f}%)\")\n","\n","    return indices[selected]\n","\n","def convertir_t3f_a_ttml(t3f_tensor):\n","    \"\"\"\n","    Convierte un objeto t3f.TensorTrain a un objeto ttml.TensorTrain.\n","\n","    Args:\n","        t3f_tensor: El objeto t3f.TensorTrain a convertir.\n","\n","    Returns:\n","        ttml.TensorTrain or None: El objeto ttml.TensorTrain convertido,\n","                                  o None si la conversión falla o la entrada es None.\n","    \"\"\"\n","    # Verificar si la entrada es válida\n","    if t3f_tensor is None:\n","        print(\"Error: El objeto t3f.TensorTrain de entrada es None. No se puede convertir.\")\n","        return None\n","\n","    print(\"\\n--- Proceso de Conversión de t3f a ttml ---\")\n","\n","    # Extraer los cores del objeto t3f.TensorTrain\n","    t3f_cores = t3f_tensor.tt_cores\n","\n","    # Convertir los tensores de TensorFlow a arrays de NumPy\n","    ttml_cores = [core.numpy() for core in t3f_cores]\n","\n","    # Crear un nuevo objeto ttml.TensorTrain con los cores convertidos\n","    try:\n","        ttml_tt = TensorTrain(cores=ttml_cores, mode='l', is_orth=False)\n","        print(\"Creación del objeto ttml.TensorTrain exitosa.\")\n","\n","        # Verificación opcional: Imprimir propiedades\n","        print(\"\\nPropiedades del ttml.TensorTrain convertido:\")\n","        print(f\"  Dims: {ttml_tt.dims}\")\n","        print(f\"  TT Rank: {ttml_tt.tt_rank}\")\n","\n","        return ttml_tt\n","\n","    except Exception as e:\n","        print(f\"Error al crear el objeto ttml.TensorTrain: {e}\")\n","        return None\n","\n","import numpy as np\n","import tensorflow as tf\n","import t3f\n","\n","# Assuming ttml.tensor_train.TensorTrain is available for type checking\n","try:\n","    from ttml.tensor_train import TensorTrain as TTMLTensorTrain\n","except ImportError:\n","    # Define a dummy class if ttml is not installed, to prevent errors\n","    # if you only want to use the t3f path or test the structure.\n","    class TTMLTensorTrain:\n","        pass\n","    print(\"Warning: ttml.TensorTrain not found. The TTML path in the functions might not work correctly.\")\n","\n","def _calculate_tt_approximation_error_value(\n","    tt_approx_model,\n","    test_indices_raw, # Could be tf.Tensor or np.ndarray\n","    test_true_values_raw, # Could be tf.Tensor or np.ndarray\n","    metric: str = 'relative_l2'\n",") -> float:\n","    \"\"\"\n","    Calculates the error of a TT approximation against true values for specified indices.\n","    This function performs the calculation without any printing. It supports ttml.TensorTrain\n","    and t3f.TensorTrain models.\n","\n","    Args:\n","        tt_approx_model: The ttml.TensorTrain or t3f.TensorTrain approximation model.\n","        test_indices_raw: The multi-indices of the test set. Can be tf.Tensor or np.ndarray.\n","        test_true_values_raw: The true values corresponding to test_indices. Can be tf.Tensor or np.ndarray.\n","        metric (str): The error metric to use ('RMSE', 'relative_l1', 'relative_l2').\n","\n","    Returns:\n","        float: The calculated overall error value.\n","    \"\"\"\n","    # Convert true values to NumPy for consistent error calculation\n","    if tf.is_tensor(test_true_values_raw):\n","        true_values = test_true_values_raw.numpy()\n","    else:\n","        true_values = test_true_values_raw\n","\n","    # Determine the type of the TT model and prepare indices accordingly\n","    estimated_values = None\n","    if isinstance(tt_approx_model, TTMLTensorTrain): # Check for ttml.TensorTrain\n","        # ttml.gather expects NumPy array indices\n","        if tf.is_tensor(test_indices_raw):\n","            test_indices_np = test_indices_raw.numpy()\n","        else:\n","            test_indices_np = test_indices_raw\n","        estimated_values = tt_approx_model.gather(test_indices_np)\n","        #print(\"Using ttml.TensorTrain.gather()\")\n","    elif isinstance(tt_approx_model, t3f.TensorTrain): # Check for t3f.TensorTrain\n","        # t3f.gather_nd expects TensorFlow Tensor indices\n","        if not tf.is_tensor(test_indices_raw):\n","            test_indices_tf = tf.constant(test_indices_raw, dtype=tf.int32)\n","        else:\n","            test_indices_tf = tf.cast(test_indices_raw, dtype=tf.int32) # Ensure int32\n","\n","        # t3f.gather_nd returns a tf.Tensor, convert to numpy for calculations\n","        estimated_values = t3f.gather_nd(tt_approx_model, test_indices_tf).numpy()\n","        #print(\"Using t3f.gather_nd()\")\n","    else:\n","        raise TypeError(\"Unsupported TT approximation model type. Must be ttml.TensorTrain or t3f.TensorTrain.\")\n","\n","    # Ensure consistent shapes\n","    if estimated_values.shape != true_values.shape:\n","        if true_values.ndim > 1 and true_values.shape[1] == 1:\n","            true_values = true_values.flatten()\n","        elif estimated_values.ndim > 1 and estimated_values.shape[1] == 1:\n","            estimated_values = estimated_values.flatten()\n","\n","        if estimated_values.shape != true_values.shape:\n","            # Handle shape mismatch by truncating to the minimum length\n","            print(f\"Warning: Shape mismatch detected for error calculation. Truncating to min length. Estimated: {estimated_values.shape}, True: {true_values.shape}\")\n","            min_len = min(len(estimated_values), len(true_values))\n","            estimated_values = estimated_values[:min_len]\n","            true_values = true_values[:min_len]\n","\n","    # Calculate error based on metric\n","    overall_error = np.inf\n","    if metric == 'RMSE':\n","        squared_errors = (estimated_values - true_values) ** 2\n","        overall_error = np.sqrt(np.mean(squared_errors))\n","    elif metric == 'relative_l1':\n","        error_values = np.abs(estimated_values - true_values)\n","        denominator = np.maximum(np.abs(true_values), 1e-12)\n","        relative_errors = error_values / denominator\n","        overall_error = np.mean(relative_errors)\n","    elif metric == 'relative_l2':\n","        overall_error = calculate_relative_l2_error(true_values, estimated_values)\n","\n","    return overall_error\n","\n","# Definir códigos de color ANSI (repetidos aquí para asegurar que estén disponibles en el scope de _print_error_metrics)\n","COLOR_BLUE = \"\\033[94m\"\n","COLOR_GREEN = \"\\033[92m\"\n","COLOR_RESET = \"\\033[0m\"\n","\n","\n","def _print_error_metrics(\n","    best_tt_approx_model, # Renamed to be generic\n","    indices,\n","    true_values,\n","    metric: str,\n","    data_set_name: str,\n","    show_example_values: bool\n","):\n","    \"\"\"\n","    Helper function to calculate and print error metrics for a given dataset.\n","    Includes an option to show example values. Supports ttml.TensorTrain and t3f.TensorTrain.\n","    \"\"\"\n","    # Convert inputs to NumPy if they are tf.Tensor for consistent handling in printing\n","    if tf.is_tensor(indices):\n","        indices_np = indices.numpy()\n","    else:\n","        indices_np = indices\n","\n","    if tf.is_tensor(true_values):\n","        true_values_np = true_values.numpy()\n","    else:\n","        true_values_np = true_values\n","\n","    # Get the overall error using the helper function (which now handles model type)\n","    overall_error = _calculate_tt_approximation_error_value(\n","        best_tt_approx_model, indices_np, true_values_np, metric # Pass raw (converted to numpy) indices here for _calculate\n","    )\n","\n","    # Recalculate estimated_values for printing examples and percentiles based on model type\n","    estimated_values_for_print = None\n","    if isinstance(best_tt_approx_model, TTMLTensorTrain):\n","        estimated_values_for_print = best_tt_approx_model.gather(indices_np)\n","    elif isinstance(best_tt_approx_model, t3f.TensorTrain):\n","        # For t3f, ensure indices are tf.Tensor\n","        if not tf.is_tensor(indices):\n","            indices_tf = tf.constant(indices, dtype=tf.int32)\n","        else:\n","            indices_tf = tf.cast(indices, dtype=tf.int32) # Ensure int32\n","        estimated_values_for_print = t3f.gather_nd(best_tt_approx_model, indices_tf).numpy()\n","    else:\n","        raise TypeError(\"Unsupported TT approximation model type for printing. Must be ttml.TensorTrain or t3f.TensorTrain.\")\n","\n","    error_values_for_print = np.abs(estimated_values_for_print - true_values_np)\n","\n","    # Ensure consistent shapes for printing\n","    if estimated_values_for_print.shape != true_values_np.shape:\n","        if true_values_np.ndim > 1 and true_values_np.shape[1] == 1:\n","            true_values_np = true_values_np.flatten()\n","        elif estimated_values_for_print.ndim > 1 and estimated_values_for_print.shape[1] == 1:\n","            estimated_values_for_print = estimated_values_for_print.flatten()\n","\n","        if estimated_values_for_print.shape != true_values_np.shape:\n","            print(f\"Warning: Shapes of estimated values for printing ({estimated_values_for_print.shape}) and true values ({true_values_np.shape}) do not match for {data_set_name}. This might affect example prints.\")\n","            min_len = min(len(estimated_values_for_print), len(true_values_np))\n","            estimated_values_for_print = estimated_values_for_print[:min_len]\n","            true_values_np = true_values_np[:min_len]\n","            error_values_for_print = error_values_for_print[:min_len]\n","\n","    metric_name = \"RMSE (Root Mean Squared Error)\" if metric == 'RMSE' else \\\n","                  \"Relative L1 Error\" if metric == 'relative_l1' else \\\n","                  \"Relative L2 Error (Norm)\"\n","\n","    print(f\"\\n--- Results for {data_set_name} ({len(indices_np)} points) ---\")\n","\n","    # --- Print individual results (if show_example_values is True) ---\n","    if show_example_values:\n","        print(f\"\\nExample results from {data_set_name}:\")\n","        for i in range(min(10, len(indices_np))):\n","            print(f\"Index: {indices_np[i]}, \"\n","                  f\"Estimated: {format_value_dynamically(estimated_values_for_print[i])}, \"\n","                  f\"Actual: {format_value_dynamically(true_values_np[i])}, \"\n","                  f\"Absolute Error: {format_value_dynamically(error_values_for_print[i])}\")\n","\n","    # --- Aggregate statistics ---\n","    print(f\"\\nSummary of {metric_name} for {data_set_name}:\")\n","\n","    # Determine color based on data_set_name\n","    color_code = COLOR_BLUE if \"Training Set\" in data_set_name else COLOR_GREEN\n","\n","    if metric == 'RMSE':\n","        print(f\"{color_code}Total {metric_name}: {format_value_dynamically(overall_error)}{COLOR_RESET}\")\n","        print(\"(Note: RMSE is a global metric; per-point error percentiles are not typically calculated for RMSE)\")\n","    elif metric == 'relative_l2':\n","        print(f\"{color_code}Total {metric_name}: {overall_error:.10e}{COLOR_RESET}\")\n","        print(\"(Note: Relative L2 Error is a global norm metric; per-point error percentiles are not typically calculated for this value)\")\n","    elif metric == 'relative_l1':\n","        denominator = np.maximum(np.abs(true_values_np), 1e-12)\n","        individual_errors_to_print = error_values_for_print / denominator\n","        print(f\"{color_code}Max {metric_name} per point: {format_value_dynamically(np.max(individual_errors_to_print))}{COLOR_RESET}\")\n","        print(f\"{color_code}Mean {metric_name} per point: {format_value_dynamically(np.mean(individual_errors_to_print))}{COLOR_RESET}\")\n","        print(f\"{color_code}Median {metric_name} per point: {format_value_dynamically(np.median(individual_errors_to_print))}{COLOR_RESET}\")\n","        print(f\"{color_code}90th Percentile of {metric_name} per point: {format_value_dynamically(np.percentile(individual_errors_to_print, 90))}{COLOR_RESET}\")\n","        print(f\"{color_code}99th Percentile of {metric_name} per point: {format_value_dynamically(np.percentile(individual_errors_to_print, 99))}{COLOR_RESET}\")\n","\n","def check_approximation_accuracy(\n","    best_tt_approx_model, # Renamed to be generic\n","    Omega,    # Training indices\n","    A_Omega,    # Training true values\n","    test_indices,\n","    test_true_values,\n","    metric: str = 'relative_l2',\n","    show_example_values: bool = False\n","):\n","    \"\"\"\n","    Compares the Tensor Train approximation against the ground truth\n","    using both the training and test set points, and calculates and prints the error.\n","    Automatically handles conversion of input indices and values from tf.Tensor to np.ndarray.\n","    This function supports both ttml.TensorTrain and t3f.TensorTrain models.\n","\n","    Args:\n","        best_tt_approx_model: The ttml.TensorTrain or t3f.TensorTrain approximation model.\n","        Omega: The multi-indices of the training set. Can be tf.Tensor or np.ndarray.\n","        A_Omega: The true values corresponding to Omega. Can be tf.Tensor or np.ndarray.\n","        test_indices: The multi-indices of the test set (Omega_C). Can be tf.Tensor or np.ndarray.\n","        test_true_values: The true values corresponding to test_indices (A_Omega_C). Can be tf.Tensor or np.ndarray.\n","        metric (str, optional): The error metric to use.\n","                                 Possible values: 'RMSE', 'relative_l1', 'relative_l2'.\n","                                 Defaults to 'relative_l2'.\n","        show_example_values (bool, optional): If True, prints a few example estimated vs. actual values\n","                                               and their absolute errors. Defaults to False.\n","\n","    Prints:\n","        Errors and summary statistics for both training and test sets, including percentiles.\n","    \"\"\"\n","    # Initial check for valid metric\n","    if metric not in ['RMSE', 'relative_l1', 'relative_l2']:\n","        raise ValueError(f\"Unsupported metric: {metric}. Choose from 'RMSE', 'relative_l1', or 'relative_l2'.\")\n","\n","    # Initial check for supported model type\n","    if not isinstance(best_tt_approx_model, (TTMLTensorTrain, t3f.TensorTrain)):\n","        raise TypeError(\"Unsupported TT approximation model type. 'best_tt_approx_model' must be an instance of ttml.TensorTrain or t3f.TensorTrain.\")\n","\n","    print(f\"\\n--- Checking Approximation Accuracy for Model Type: {type(best_tt_approx_model).__name__} ---\")\n","\n","    # Calculate and print metrics for the Training Set\n","    _print_error_metrics(\n","        best_tt_approx_model,\n","        Omega,\n","        A_Omega,\n","        metric,\n","        \"Training Set\",\n","        show_example_values\n","    )\n","\n","    # Calculate and print metrics for the Test Set\n","    _print_error_metrics(\n","        best_tt_approx_model,\n","        test_indices,\n","        test_true_values,\n","        metric,\n","        \"Test Set\",\n","        show_example_values\n","    )"]},{"cell_type":"markdown","source":["# TT-Cross"],"metadata":{"id":"5AtK-_1Ug7Hr"}},{"cell_type":"code","source":["# --- Explicación de la limitación ---\n","# En un entorno real con el paquete ttml instalado, podrías usar inspect.getsource\n","# de la siguiente manera para obtener el código fuente:\n","#\n","import ttml.tt_cross\n","import inspect\n","\n","try:\n","    source_init = inspect.getsource(ttml.tt_cross._compute_multi_indices)\n","    print(source_init)\n","except AttributeError:\n","    print(\"No se pudo encontrar _init_tt_cross en ttml.tt_cross.\")\n","\n","#Debido a que ttml no está instalado aquí, proporcionaremos implementaciones conceptuales."],"metadata":{"id":"ihzowW_YzFtt","executionInfo":{"status":"ok","timestamp":1749558258649,"user_tz":-120,"elapsed":10,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}},"outputId":"2cdb1d84-ccfb-4bdd-afa4-6342cc02e205","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["def _compute_multi_indices(ind, ind_old, direction):\n","    \"\"\"\n","    Compute new multiindex from old multiindex and pairs of (alpha_{k-1},i_k) as\n","    described in Savistyanov-Oseledets. This guarantees a nested sequence of\n","    multiindices, and works for both the left and right indices.\n","    \"\"\"\n","    r = ind.shape[1]\n","    if direction == \"RL\":\n","        dim_indices, previous_indices = ind\n","    else:\n","        previous_indices, dim_indices = ind\n","\n","    if ind_old is None:\n","        return dim_indices.reshape(1, r)\n","    else:\n","        ind_new = np.zeros((len(ind_old) + 1, r), dtype=np.int32)\n","        if direction == \"RL\":\n","            ind_new[1:, :] = ind_old[:, previous_indices]\n","            ind_new[0, :] = dim_indices\n","        elif direction == \"LR\":\n","            ind_new[:-1, :] = ind_old[:, previous_indices]\n","            ind_new[-1, :] = dim_indices\n","        else:\n","            raise ValueError(\"Direction has to be 'LR' or 'RL'\")\n","    return ind_new\n","\n"]}]},{"cell_type":"code","source":["# --- Explicación de la limitación ---\n","# En un entorno real con el paquete ttml instalado, podrías usar inspect.getsource\n","# de la siguiente manera para obtener el código fuente:\n","#\n","import ttml.tt_cross\n","import inspect\n","\n","try:\n","    source_init = inspect.getsource(ttml.tt_cross.maxvol)\n","    print(source_init)\n","except AttributeError:\n","    print(\"No se pudo encontrar _init_tt_cross en ttml.tt_cross.\")\n","\n","#Debido a que ttml no está instalado aquí, proporcionaremos implementaciones conceptuales."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGsKBGCus9Ts","executionInfo":{"status":"ok","timestamp":1749556717693,"user_tz":-120,"elapsed":9,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}},"outputId":"eef8cefe-6526-4bad-ab84-f93c5448b7bf"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["def maxvol(A, eps=1e-2, niters=100):\n","    \"\"\"\n","    Quasi-max volume submatrix\n","\n","    Initializes with pivot indices of LU decomposition, then greedily\n","    interchanges rows.\n","    \"\"\"\n","    n, r = A.shape\n","    if n <= r:\n","        return np.arange(n)\n","    A, _ = scipy.linalg.qr(A, mode=\"economic\")\n","    out = scipy.linalg.lapack.dgetrf(A)  # LU decomp\n","\n","    _, P, _ = out\n","    ind = _piv_to_ind(P, n)[:r]\n","\n","    sbm = A[ind[:r]]\n","    b = _right_solve(sbm, A)\n","\n","    for _ in range(niters):\n","        i0, j0 = np.unravel_index(np.argmax(np.abs(b)), b.shape)\n","        mx0 = b[i0, j0]\n","        if np.abs(mx0) <= 1 + eps:\n","            break\n","        k = ind[j0]\n","        b += np.outer(b[:, j0], b[k, :] - b[i0, :]) / mx0\n","        ind[j0] = i0\n","    ind.sort()\n","    return ind\n","\n"]}]},{"cell_type":"code","source":["# --- Explicación de la limitación ---\n","# En un entorno real con el paquete ttml instalado, podrías usar inspect.getsource\n","# de la siguiente manera para obtener el código fuente:\n","#\n","import ttml.tt_cross\n","import inspect\n","\n","try:\n","    source_init = inspect.getsource(ttml.tt_cross._maxvol_tensor)\n","    print(source_init)\n","except AttributeError:\n","    print(\"No se pudo encontrar _init_tt_cross en ttml.tt_cross.\")\n","\n","#Debido a que ttml no está instalado aquí, proporcionaremos implementaciones conceptuales."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwZGGXtms6K9","executionInfo":{"status":"ok","timestamp":1749557965991,"user_tz":-120,"elapsed":27,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}},"outputId":"1d25bbbb-f5af-4246-843e-580819ab69f9"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["def _maxvol_tensor(X, mu, transpose=False):\n","    \"\"\"\n","    Matricize `X` with respect to mode `mu` and return maxvol submatrix and\n","    indices\n","    \"\"\"\n","    permutation = np.concatenate(\n","        [np.arange(mu), np.arange(mu + 1, len(X.shape)), [mu]]\n","    )\n","    Y = X.transpose(permutation)\n","    Y = Y.reshape(-1, X.shape[mu])\n","    ind = maxvol(Y)\n","    R = Y[ind, :]\n","    ind = np.unravel_index(ind, X.shape[:mu] + X.shape[mu + 1 :])\n","    ind = np.stack(ind)\n","    if transpose:\n","        R = R.T\n","    return ind, R\n","\n"]}]},{"cell_type":"code","source":["# --- Explicación de la limitación ---\n","# En un entorno real con el paquete ttml instalado, podrías usar inspect.getsource\n","# de la siguiente manera para obtener el código fuente:\n","#\n","import ttml.tt_cross\n","import inspect\n","\n","try:\n","    source_init = inspect.getsource(ttml.tt_cross._init_tt_cross)\n","    print(source_init)\n","except AttributeError:\n","    print(\"No se pudo encontrar _init_tt_cross en ttml.tt_cross.\")\n","\n","#Debido a que ttml no está instalado aquí, proporcionaremos implementaciones conceptuales."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJa8dL9SpAZy","executionInfo":{"status":"ok","timestamp":1749557971594,"user_tz":-120,"elapsed":5,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}},"outputId":"a0cd7958-f0a2-4363-a79e-5993ace90e86"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["def _init_tt_cross(tt):\n","    \"\"\"Generate initial set of R-matrices and right-indices for TT-cross.\n","\n","    Same for DMRG as for regular TT-cross. This version follows the paper\n","    instead of Matlab code.\"\"\"\n","    tt.orthogonalize(\"l\")\n","    nd = len(tt)\n","    P_mats = [None] * (nd + 1)\n","    P_mats[0] = np.array([[1]])\n","    P_mats[-1] = np.array([[1]])\n","    index_array = [None] * (nd + 1)\n","    R = np.array([[1]])\n","\n","    for i in range(nd - 1, 0, -1):\n","        core = tt[i]\n","        core = np.einsum(\"ijk,kl->ijl\", core, R)\n","\n","        # RQ decomposition of core\n","        Q, R = _qr_tensor(core, 0, True)\n","\n","        tt[i] = Q\n","        Q = np.einsum(\"ijk,kl\", Q, P_mats[i + 1])\n","\n","        # Max vol indices\n","        # ind = maxvol(core.T)\n","        ind, P = _maxvol_tensor(Q, 0, True)\n","        P_mats[i] = P\n","\n","        # Compute new indices from previous and maxvol\n","        ind_new = _compute_multi_indices(ind, index_array[i + 1], \"RL\")\n","        index_array[i] = ind_new\n","    tt[0] = np.einsum(\"ijk,kl->ijl\", tt[0], R)\n","\n","    # tt is now right-orthogonalized and identical to what we started with\n","    tt.mode = 0\n","    return tt, P_mats, index_array\n","\n"]}]},{"cell_type":"code","source":["# --- Explicación de la limitación ---\n","# En un entorno real con el paquete ttml instalado, podrías usar inspect.getsource\n","# de la siguiente manera para obtener el código fuente:\n","#\n","import ttml.tt_cross\n","import inspect\n","\n","try:\n","    source_init = inspect.getsource(ttml.tt_cross._sweep_step_regular)\n","    print(\"--- _init_tt_cross Source ---\")\n","    print(source_init)\n","except AttributeError:\n","    print(\"No se pudo encontrar _init_tt_cross en ttml.tt_cross.\")\n","\n","#Debido a que ttml no está instalado aquí, proporcionaremos implementaciones conceptuales."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCzhXxYwp5Se","executionInfo":{"status":"ok","timestamp":1749555865531,"user_tz":-120,"elapsed":15,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}},"outputId":"b0c1df5e-7667-465a-9257-cc5a27d35461"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["--- _init_tt_cross Source ---\n","def _sweep_step_regular(\n","    i, direction, tt, index_array, index_fun, Pmats, cache=None, verbose=False\n","):\n","    \"\"\"\n","    Do one step of the DMRG TT-cross algorithm, sweeping in a specified\n","    direction.\n","\n","    Parameters\n","    ----------\n","    i : int\n","        Left index of the supercore\n","    direction : str\n","        Either \"LR\" or \"RL\", corresponding to a sweep in the left-to-right\n","        and right-to-left direcition respectively\n","    tt : TensorTrain\n","        TensorTrain to be modified\n","    index_array : list[np.ndarray]\n","        list of left_indices and right_indices. At step `i`, `index_array[i+1]`\n","        will be modified\n","    index_fun : function\n","        Function mapping indices to function values to be used for fitting\n","    Pmats : list[np.ndarray]\n","        The list of matrices to be used to compute maxvol at step i. At step\n","        `i`, `Pmats[i+1]` will be modified.\n","    verbose: bool (default: False)\n","        Print convergence information every step.\n","    \"\"\"\n","\n","    # Compute indices used to sample supercore\n","    left_inds = index_array[i]\n","    right_inds = index_array[i + 1]\n","    big_ind = _core_index(left_inds, right_inds, tt.dims[i])\n","\n","    core = index_fun(big_ind)\n","    if cache is not None:\n","        cache[\"func_vals\"] = np.concatenate(\n","            [cache[\"func_vals\"], core.reshape(-1)]\n","        )\n","        cache[\"inds\"] = np.concatenate(\n","            [cache[\"inds\"], big_ind.reshape(-1, len(tt))]\n","        )\n","\n","    # Multiply core by (pseudo)inverse of the P-matrices on the left and\n","    # right edge.\n","    core = _apply_inv_matrix(core, Pmats[i], 0)\n","    core = _apply_inv_matrix(core, Pmats[i + 1].T, 2)\n","\n","    # Orthogonalize and absorb R-factor into next core\n","    if direction == \"LR\" and (i < len(tt) - 1):\n","        core, R = _qr_tensor(core, 2, False)\n","        tt[i + 1] = np.einsum(\"ij,jkl->ikl\", R, tt[i + 1])\n","    elif direction == \"RL\" and (i > 0):\n","        core, R = _qr_tensor(core, 0, True)\n","        tt[i - 1] = np.einsum(\"ijk,kl->ijl\", tt[i - 1], R)\n","\n","    tt[i] = core\n","\n","    # Use maxvol to compute new P matrix and nested multi-indices\n","    if direction == \"LR\" and i < len(tt) - 1:\n","        A = np.einsum(\"ij,jkl->ikl\", Pmats[i], core)\n","        ind, P = _maxvol_tensor(A, 2)\n","        Pmats[i + 1] = P\n","        new_indices = _compute_multi_indices(ind, left_inds, \"LR\")\n","        index_array[i + 1] = new_indices\n","\n","    if direction == \"RL\" and i > 0:\n","        A = np.einsum(\"ijk,kl\", core, Pmats[i + 1])\n","        ind, P = _maxvol_tensor(A, 0, True)\n","        Pmats[i] = P\n","        new_indices = _compute_multi_indices(ind, right_inds, \"RL\")\n","        index_array[i] = new_indices\n","\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import time\n","import tensorflow as tf\n","from ttml.tt_cross import random_idx, _init_tt_cross, _sweep_step_regular, _sweep_step_dmrg, index_function_wrapper # Assuming _sweep_step_dmrg exists\n","from ttml.tensor_train import TensorTrain # Assuming this is your TensorTrain class\n","\n","def collecting_index_fun(original_index_fun):\n","    \"\"\"Wrapper que simplemente pasa los índices a la función original y retorna su resultado.\n","       LA RECOLECCIÓN NO OCURRE AQUÍ.\"\"\"\n","    def wrapped_index_fun(indices):\n","        # No global collection happens in this function anymore.\n","        # Just pass through indices and return original_index_fun's result.\n","        # Let original_index_fun (which calls physical_point_index_fun) handle input shape and collection.\n","        return original_index_fun(indices)\n","\n","    return wrapped_index_fun\n","\n","# --- Merged tt_cross_regular_v2 (uses _sweep_step_regular) ---\n","def tt_cross_regular_v2(\n","    tt,\n","    index_fun,\n","    Omega_C,  # Índices del test set\n","    A_Omega_C,  # Valores verdaderos del test set\n","    tol_flattening: float = 1e-2,\n","    tol_precision: float = 1e-4,\n","    metric: str = 'relative_l2',  # 'rmse' o 'relative_l2'\n","    max_its: int = 10,\n","    verbose: bool = False,\n","    inplace: bool = True\n","):\n","    \"\"\"\n","    Implements TT-Cross algorithm with selectable precision stopping criterion (RMSE or Relative L2).\n","    Uses an external validation set provided as Omega_C and A_Omega_C.\n","    \"\"\"\n","    if metric not in ['rmse', 'relative_l2']:\n","        raise ValueError(f\"Unsupported metric: {metric}. Choose 'rmse' or 'relative_l2'.\")\n","    error_metric_name = \"RMSE\" if metric == 'rmse' else \"Relative L2 Error\"\n","\n","    if not inplace:\n","        tt = tt.copy()\n","    tt, Pmats, index_array = _init_tt_cross(tt)\n","    direction = \"LR\"\n","\n","    # Usar el conjunto de validación proporcionado\n","    cache = {\n","        \"inds\": Omega_C,\n","        \"func_vals\": A_Omega_C\n","    }\n","\n","    errors = []\n","    stop_reason = \"max_iterations\"\n","    iterations_completed = 0\n","\n","    for j in range(max_its):\n","        if verbose:\n","            print(f\"Sweep {j}, direction {'LR' if direction == 'LR' else 'RL'}. Algorithm: tt_cross_regular_v2\")\n","\n","        if direction == \"LR\":\n","            for i in range(len(tt)):\n","                _sweep_step_regular(\n","                    i, \"LR\", tt, index_array, index_fun, Pmats, cache, verbose\n","                )\n","            direction = \"RL\"\n","        else:\n","            for i in range(len(tt) - 1, -1, -1):\n","                _sweep_step_regular(\n","                    i, \"RL\", tt, index_array, index_fun, Pmats, cache, verbose\n","                )\n","            direction = \"LR\"\n","        iterations_completed = j + 1\n","\n","        # Calcular el error usando el conjunto de validación\n","        y_pred = tt.gather(cache[\"inds\"])\n","        y_true = cache[\"func_vals\"]\n","\n","        current_error = np.inf\n","        if y_true.size > 0 and y_pred.size == y_true.size:\n","            if metric == 'rmse':\n","                current_error = np.sqrt(np.mean((y_pred - y_true) ** 2))\n","            elif metric == 'relative_l2':\n","                current_error = calculate_relative_l2_error(y_true, y_pred)\n","        errors.append(current_error)\n","\n","        if verbose:\n","            print(f\"Last {error_metric_name} (from validation set): {current_error:.10e}\")\n","\n","        if tol_precision is not None and current_error < tol_precision:\n","            stop_reason = \"precision\"\n","            break\n","\n","        if tol_flattening is not None and len(errors) > 3:\n","            max_prev_error = np.max(errors[-4:-1])\n","            if max_prev_error > 1e-12 and max_prev_error != np.inf:\n","                change = (errors[-1] - max_prev_error) / max_prev_error\n","                if change > -tol_flattening:\n","                    stop_reason = \"flattening\"\n","                    break\n","            elif errors[-1] == max_prev_error:\n","                stop_reason = \"flattening\"\n","                break\n","\n","    tt.orthogonalize()\n","    tt.errors = np.array(errors)\n","    return tt, stop_reason, iterations_completed, cache\n","\n","\n","# --- Modified tt_cross_regular_v2_dmrg_step (uses _sweep_step_dmrg) ---\n","def tt_cross_regular_v2_dmrg_step(\n","    tt,\n","    index_fun,\n","    tol_flattening: float = 1e-2,\n","    tol_precision: float = 1e-3,\n","    metric: str = 'relative_l2', # New: 'rmse' or 'relative_l2'\n","    max_its: int = 10,\n","    verbose: bool = False,\n","    inplace: bool = True,\n","    rank_kick: int = 0\n","):\n","    \"\"\"\n","    Implements TT-Cross algorithm with DMRG sweep steps (_sweep_step_dmrg)\n","    and selectable precision/flattening stopping criteria (RMSE or Relative L2).\n","    \"\"\"\n","    if metric not in ['rmse', 'relative_l2']:\n","        raise ValueError(f\"Unsupported metric: {metric}. Choose 'rmse' or 'relative_l2'.\")\n","    error_metric_name = \"RMSE\" if metric == 'rmse' else \"Relative L2 Error\"\n","\n","    if not inplace:\n","        tt = tt.copy()\n","\n","    tt, Pmats, index_array = _init_tt_cross(tt)\n","    direction = \"LR\"\n","\n","    cache = dict()\n","    cache_inds_batch = random_idx(tt, 200)\n","    cache[\"inds\"] = cache_inds_batch\n","\n","    func_vals_initial = index_fun(cache_inds_batch)\n","    if func_vals_initial is not None and func_vals_initial.size > 0 :\n","        cache[\"func_vals\"] = func_vals_initial.reshape(-1)\n","    else:\n","        cache[\"func_vals\"] = np.array([]) # Start with empty if no values returned\n","\n","    errors = []\n","    stop_reason = \"max_iterations\"\n","    iterations_completed = 0\n","\n","    for j in range(max_its):\n","        if verbose:\n","            print(f\"Sweep {j}, direction {'LR' if direction == 'LR' else 'RL'}. Algorithm: tt_cross_regular_v2_dmrg_step (Metric: {error_metric_name})\")\n","\n","        if direction == \"LR\":\n","            for i in range(len(tt) - 1): # DMRG sweep usually up to d-2\n","                _sweep_step_dmrg( # Uses DMRG sweep step\n","                    i, \"LR\", tt, index_array, index_fun, Pmats, rank_kick=rank_kick, verbose=verbose, cache=cache\n","                )\n","            direction = \"RL\"\n","        else:\n","            for i in range(len(tt) - 2, -1, -1): # DMRG sweep usually d-2 down to 0\n","                _sweep_step_dmrg( # Uses DMRG sweep step\n","                    i, \"RL\", tt, index_array, index_fun, Pmats, rank_kick=rank_kick, verbose=verbose, cache=cache\n","                )\n","            direction = \"LR\"\n","        iterations_completed = j + 1\n","\n","        current_error_value = np.inf\n","        if cache[\"inds\"].shape[0] > 0 and cache[\"func_vals\"].size > 0:\n","            y_pred = tt.gather(cache[\"inds\"])\n","            y_true = cache[\"func_vals\"]\n","\n","            min_len = min(len(y_pred), len(y_true))\n","            if min_len > 0:\n","                y_pred_m, y_true_m = y_pred[:min_len], y_true[:min_len]\n","                if metric == 'rmse':\n","                    current_error_value = np.sqrt(np.mean((y_pred_m - y_true_m) ** 2))\n","                elif metric == 'relative_l2':\n","                    current_error_value = calculate_relative_l2_error(y_true_m, y_pred_m)\n","                errors.append(current_error_value)\n","            else:\n","                errors.append(np.inf) # Cannot calculate error\n","        else:\n","            errors.append(np.inf) # Cache empty or incomplete\n","\n","        current_error_value = errors[-1] # Get the error just calculated\n","\n","        if verbose:\n","            print(f\"Last {error_metric_name} (from cache): {current_error_value:.10e}\")\n","\n","        if tol_precision is not None and current_error_value < tol_precision:\n","            stop_reason = \"precision\"\n","            break\n","\n","        if tol_flattening is not None and len(errors) > 3:\n","            max_prev_error = np.max(errors[-4:-1])\n","            if max_prev_error > 1e-12 and max_prev_error != np.inf: # Avoid division by zero/inf\n","                change = (current_error_value - max_prev_error) / max_prev_error\n","                if change > -tol_flattening:\n","                    stop_reason = \"flattening\"\n","                    break\n","            elif current_error_value == max_prev_error : # Handles cases where errors are tiny or stuck\n","                 stop_reason = \"flattening\"\n","                 break\n","\n","\n","    tt.orthogonalize()\n","    tt.errors = np.array(errors)\n","    return tt, stop_reason, iterations_completed, cache\n","\n","# Global variables (assuming these are managed externally as in your original code)\n","total_vector_evaluations = 0\n","collected_indices = []\n","collected_evaluations = []\n","\n","def optimize_tt_cross_rank_sweep(\n","    d,\n","    MODAL_SIZE,\n","    sizeOmega, # Límite de evaluaciones para la selección automática\n","    min_rank_to_try,\n","    max_rank_to_try,\n","    seed,\n","    tol_flattening,\n","    max_its,\n","    tol_precision,\n","    physical_point_index_fun,\n","    collecting_index_fun,\n","    tt_cross_algorithm_func,\n","    create_tt_random,\n","    create_tt_initial, # No se usa directamente en esta versión, pero se mantiene por consistencia de la firma\n","    get_tt_shape,\n","    TensorTrain, # Clase TensorTrain, si es necesaria para instanciar o type-hinting\n","    index_function_wrapper,\n","    Omega_C,    # Conjunto de validación (índices)\n","    A_Omega_C,    # Valores verdaderos del conjunto de validación\n","    # --- Funciones helper que deben estar definidas en el scope ---\n","    _calculate_tt_approximation_error_value, # Función para calcular error\n","    process_and_verify, # Función para procesar índices/evaluaciones\n","    # --- Parámetros adicionales ---\n","    dmrg_rank_kick: int = 0,\n","    metric: str = 'relative_l2',\n","    manual_rank: int = None,\n","):\n","    \"\"\"\n","    Realiza un barrido de optimización TT-Cross sobre diferentes rangos utilizando un algoritmo TT-Cross\n","    especificado y una métrica de error. Utiliza un conjunto de validación externo (Omega_C, A_Omega_C).\n","    Al final, procesa los índices y evaluaciones recolectados para el mejor TT y calcula\n","    el error tanto en el conjunto de validación como en este conjunto de entrenamiento procesado.\n","    \"\"\"\n","    global total_vector_evaluations, collected_indices, collected_evaluations\n","\n","    # Definir códigos de color ANSI\n","    COLOR_BLUE = \"\\033[94m\"\n","    COLOR_GREEN = \"\\033[92m\"\n","    COLOR_RESET = \"\\033[0m\"\n","\n","    if metric not in ['rmse', 'relative_l2']:\n","        raise ValueError(f\"Métrica no soportada: {metric}. Elija 'rmse' o 'relative_l2'.\")\n","    error_metric_name = \"RMSE\" if metric == 'rmse' else \"Error Relativo L2\"\n","\n","    NUM_INTERVALS = MODAL_SIZE - 1 # Asumo que esto es relevante para create_tt_random\n","\n","    # Configuración de la función de indexación para TT-Cross\n","    chebyshev_grid_index_fun_base = lambda indices_batch: physical_point_index_fun(indices_batch)\n","    wrapped_by_ttml_index_fun = index_function_wrapper(chebyshev_grid_index_fun_base)\n","    complex_index_fun_wrapped_physical = collecting_index_fun(wrapped_by_ttml_index_fun)\n","\n","    # Variables para la mejor aproximación automática\n","    auto_best_tt_approx = None\n","    auto_best_error_value_val = np.inf # Error en el conjunto de validación\n","    auto_best_evaluations_list = []\n","    auto_best_indices_list = []\n","    auto_best_rank_found = None\n","    auto_best_evaluations_count = 0\n","\n","    # Variables para el rango manual\n","    manual_rank_data_tuple = None\n","\n","    print(f\"Iniciando barrido de rangos para TT-Cross (Métrica: {error_metric_name})\")\n","    print(f\"Validación externa: {Omega_C.shape[0] if hasattr(Omega_C, 'shape') else len(Omega_C)} puntos.\")\n","    print(\"-\" * 30)\n","\n","    for current_rank_test in range(min_rank_to_try, max_rank_to_try + 1):\n","        print(f\"\\nProbando con rango TT objetivo: {current_rank_test}\")\n","        desired_ranks = [1] + [current_rank_test] * (d - 1) + [1]\n","\n","        # Crear TT inicial para esta prueba de rango\n","        tt_initial = create_tt_random(desired_ranks, NUM_INTERVALS, d, seed)\n","\n","        print(f\"  Rango TT inicial: {tt_initial.tt_rank}\")\n","        initial_ttml_shape = get_tt_shape(tt_initial)\n","        print(f\"  Forma TTML inicial: {initial_ttml_shape}\")\n","        expected_shape = tuple([MODAL_SIZE] * d)\n","        if initial_ttml_shape != expected_shape:\n","            print(f\"  Advertencia: La forma TTML inicial es {initial_ttml_shape}, pero se esperaba {expected_shape}.\")\n","\n","        # Reiniciar contadores de evaluación para esta ejecución de rango\n","        total_vector_evaluations = 0\n","        collected_indices = []  # Asegúrate de que tu collecting_index_fun las llene\n","        collected_evaluations = [] # Asegúrate de que tu collecting_index_fun las llene\n","\n","        start_time = time.time()\n","\n","        additional_kwargs = {}\n","        # Comprobar si la función tt_cross_algorithm_func acepta 'rank_kick'\n","        if hasattr(tt_cross_algorithm_func, '__code__') and 'rank_kick' in tt_cross_algorithm_func.__code__.co_varnames:\n","            additional_kwargs['rank_kick'] = dmrg_rank_kick\n","\n","        tt_approx = None\n","        stop_reason = \"Algoritmo no completado como se esperaba\"\n","        iterations_completed = 0\n","        # run_cache ya no es necesaria si el error se recalcula siempre al final\n","\n","        try:\n","            tt_approx, stop_reason, iterations_completed, _ = tt_cross_algorithm_func(\n","                tt=tt_initial,\n","                index_fun=complex_index_fun_wrapped_physical,\n","                Omega_C=Omega_C,      # Se pasa para el cálculo de error *interno* del algo TT-Cross\n","                A_Omega_C=A_Omega_C,    # Se pasa para el cálculo de error *interno* del algo TT-Cross\n","                tol_flattening=tol_flattening,\n","                tol_precision=tol_precision,\n","                metric=metric,        # Métrica para el criterio de parada *interno*\n","                max_its=max_its,\n","                verbose=False,        # Podrías hacerlo un parámetro de optimize_tt_cross_rank_sweep_new\n","                inplace=True,\n","                **additional_kwargs\n","            )\n","        except Exception as e:\n","            print(f\"  Error durante tt_cross_algorithm_func para rango {current_rank_test}: {e}\")\n","            # Continuar al siguiente rango si este falla\n","\n","        end_time = time.time()\n","        duration = end_time - start_time\n","        evaluations_during_run = total_vector_evaluations # Desde la variable global modificada por collecting_index_fun\n","        collected_count = len(collected_evaluations)\n","\n","        # --- RECALCULAR el error en el CONJUNTO DE VALIDACIÓN (Omega_C, A_Omega_C) ---\n","        # Se usa la métrica principal de la función (pasada como 'metric')\n","        current_run_error_value_validation = np.inf\n","        if tt_approx is not None:\n","            try:\n","                current_run_error_value_validation = _calculate_tt_approximation_error_value(\n","                    tt_approx_model=tt_approx,\n","                    test_indices_raw=Omega_C,\n","                    test_true_values_raw=A_Omega_C,\n","                    metric=metric # Usa la métrica principal definida para la optimización\n","                )\n","            except Exception as e:\n","                print(f\"  Advertencia: Error al recalcular la métrica de validación final para el rango {current_rank_test}: {e}\")\n","        else:\n","            print(f\"  Advertencia: tt_approx es None para el rango {current_rank_test}. No se puede calcular el error de validación.\")\n","\n","\n","        final_rank_of_approx = tt_approx.tt_rank if tt_approx is not None else \"N/A (tt_approx is None)\"\n","        print(f\"  Rango TT final de la aproximación: {final_rank_of_approx}\")\n","        print(f\"  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): {evaluations_during_run}\")\n","        print(f\"  Pares (índice, valor) recolectados individualmente: {collected_count}\")\n","        print(f\"  Iteraciones completadas: {iterations_completed}\")\n","        print(f\"  Razón de parada del algoritmo TT-Cross: {stop_reason}\")\n","        print(f\"  Tiempo de ejecución para este rango: {duration:.2f} segundos\")\n","        # Imprimir el error de validación en verde\n","        print(f\"  {COLOR_GREEN}{error_metric_name} (en Conjunto de Validación Omega_C, recalculado): {current_run_error_value_validation:.10e}{COLOR_RESET}\")\n","\n","        # Guardar datos si es el rango manual especificado\n","        if manual_rank is not None and current_rank_test == manual_rank:\n","            print(f\"  Guardando datos para el rango especificado manualmente: {current_rank_test}.\")\n","            manual_rank_data_tuple = (\n","                tt_approx.copy() if tt_approx is not None else None,\n","                current_run_error_value_validation, # Error en validación\n","                collected_evaluations.copy(),    # Los recolectados en esta ejecución\n","                collected_indices.copy(),        # Los recolectados en esta ejecución\n","                current_rank_test,\n","                evaluations_during_run\n","            )\n","\n","        # Actualizar la mejor aproximación automática\n","        # Criterio: tt_approx no es None, evaluaciones dentro del límite Y error de validación es el mejor hasta ahora\n","        if tt_approx is not None and \\\n","           evaluations_during_run <= sizeOmega and \\\n","           current_run_error_value_validation < auto_best_error_value_val:\n","\n","            auto_best_tt_approx = tt_approx.copy()\n","            auto_best_error_value_val = current_run_error_value_validation # Guardar el error de validación\n","            auto_best_evaluations_list = collected_evaluations.copy()\n","            auto_best_indices_list = collected_indices.copy()\n","            auto_best_rank_found = current_rank_test\n","            auto_best_evaluations_count = evaluations_during_run\n","            print(f\"  ¡Nueva mejor aproximación automática encontrada!\")\n","            print(f\"    Rango: {current_rank_test}, {error_metric_name} (Validación): {auto_best_error_value_val:.10e}, Evaluaciones: {auto_best_evaluations_count}\")\n","\n","    # --- Lógica de Selección Final ---\n","    print(f\"\\n{'-'*30}\\nSelección Final de la Aproximación TT\")\n","\n","    chosen_tt_approx = None\n","    chosen_error_value_val = np.inf # Error en el conjunto de validación para el TT elegido\n","    chosen_evaluations_list = []\n","    chosen_indices_list = []\n","    chosen_rank_found = None\n","    chosen_evaluations_count = 0\n","    selection_details_message = \"\"\n","\n","    if manual_rank is not None:\n","        if manual_rank_data_tuple is not None:\n","            print(f\"Priorizando resultados del rango especificado manualmente: {manual_rank}.\")\n","            (chosen_tt_approx, chosen_error_value_val, chosen_evaluations_list,\n","             chosen_indices_list, chosen_rank_found, chosen_evaluations_count) = manual_rank_data_tuple\n","            selection_details_message = f\"Rango seleccionado manualmente: {chosen_rank_found}\"\n","            if chosen_tt_approx is None:\n","                selection_details_message += \" (resultó en ninguna aproximación válida)\"\n","            # Nota: la restricción de sizeOmega para el rango manual es una advertencia, no un descarte.\n","            elif chosen_evaluations_count > sizeOmega:\n","                print(f\"  Advertencia: El rango manual {chosen_rank_found} usó {chosen_evaluations_count} evaluaciones, excediendo sizeOmega ({sizeOmega}).\")\n","                selection_details_message += f\" (Nota: usó {chosen_evaluations_count} evaluaciones, límite sizeOmega={sizeOmega})\"\n","        else:\n","            print(f\"Advertencia: Se especificó el rango manual {manual_rank} pero no se procesó o no está en el rango [{min_rank_to_try}, {max_rank_to_try}].\")\n","            if auto_best_tt_approx is not None:\n","                print(\"  Recurriendo a la mejor aproximación encontrada automáticamente.\")\n","                chosen_tt_approx = auto_best_tt_approx\n","                chosen_error_value_val = auto_best_error_value_val\n","                chosen_evaluations_list = auto_best_evaluations_list\n","                chosen_indices_list = auto_best_indices_list\n","                chosen_rank_found = auto_best_rank_found\n","                chosen_evaluations_count = auto_best_evaluations_count\n","                selection_details_message = f\"Rango seleccionado automáticamente: {chosen_rank_found} (rango manual {manual_rank} no procesado)\"\n","            else:\n","                selection_details_message = f\"Rango manual {manual_rank} no procesado, y no se encontró ninguna aproximación automática.\"\n","    else: # Sin rango manual, usar la mejor automática si existe\n","        if auto_best_tt_approx is not None:\n","            print(\"Usando la mejor aproximación encontrada automáticamente (no se especificó rango manual).\")\n","            chosen_tt_approx = auto_best_tt_approx\n","            chosen_error_value_val = auto_best_error_value_val\n","            chosen_evaluations_list = auto_best_evaluations_list\n","            chosen_indices_list = auto_best_indices_list\n","            chosen_rank_found = auto_best_rank_found\n","            chosen_evaluations_count = auto_best_evaluations_count\n","            selection_details_message = f\"Rango seleccionado automáticamente: {chosen_rank_found}\"\n","        else:\n","            selection_details_message = \"No se especificó rango manual, y no se encontró ninguna aproximación automática.\"\n","\n","    # --- Procesar los índices y evaluaciones del TT ELEGIDO ---\n","    Omega_processed_train = np.array([]) # Inicializar como arrays vacíos\n","    A_Omega_processed_train = np.array([]) # Inicializar como arrays vacíos\n","    if chosen_tt_approx is not None and chosen_indices_list and chosen_evaluations_list:\n","        print(f\"\\nProcesando los {len(chosen_indices_list)} pares (índice, valor) recolectados para el TT elegido...\")\n","        try:\n","            Omega_processed_train, A_Omega_processed_train = process_and_verify(\n","                chosen_indices_list,\n","                chosen_evaluations_list,\n","                MODAL_SIZE # Pasamos MODAL_SIZE para posible uso en la verificación\n","            )\n","            print(f\"  Procesamiento completado. Obtenido conjunto de entrenamiento con {Omega_processed_train.shape[0] if hasattr(Omega_processed_train, 'shape') else len(Omega_processed_train)} puntos.\")\n","        except Exception as e:\n","            print(f\"  Error durante process_and_verify: {e}. Omega_processed_train y A_Omega_processed_train permanecerán vacíos.\")\n","            # Omega_processed_train y A_Omega_processed_train ya están inicializados como vacíos\n","    elif chosen_tt_approx is not None:\n","        print(\"\\nAdvertencia: El TT elegido no tiene índices/evaluaciones recolectados para procesar (listas vacías).\")\n","        # Omega_processed_train y A_Omega_processed_train ya están inicializados como vacíos\n","\n","    # --- Resumen de la Aproximación Elegida y sus Errores ---\n","    print(f\"\\n{'-'*30}\\nResumen de la Aproximación TT Elegida\")\n","    if chosen_tt_approx is not None:\n","\n","        print(f\"Detalles de selección: {selection_details_message}\")\n","        print(f\"  Rango TT objetivo del barrido: {chosen_rank_found}\")\n","        print(f\"  Rangos TT reales de los cores: {chosen_tt_approx.tt_rank}\")\n","\n","        # Calcular error en el CONJUNTO DE ENTRENAMIENTO PROCESADO (Omega_processed_train, A_Omega_processed_train)\n","        error_on_processed_training_set = np.inf\n","        if Omega_processed_train.size > 0 and A_Omega_processed_train.size > 0:\n","            try:\n","                error_on_processed_training_set = _calculate_tt_approximation_error_value(\n","                    tt_approx_model=chosen_tt_approx,\n","                    test_indices_raw=Omega_processed_train,\n","                    test_true_values_raw=A_Omega_processed_train,\n","                    metric=metric\n","                )\n","                # Imprimir el error de entrenamiento en azul\n","                print(f\"  {COLOR_BLUE}{error_metric_name} (en Conjunto de Entrenamiento Procesado): {error_on_processed_training_set:.10e}{COLOR_RESET}\")\n","            except Exception as e:\n","                print(f\"  Advertencia: Error al calcular la métrica en el conjunto de entrenamiento procesado: {e}\")\n","        else:\n","            print(f\"  {COLOR_BLUE}{error_metric_name} (en Conjunto de Entrenamiento Procesado): N/A (conjunto vacío o no generado).{COLOR_RESET}\")\n","\n","\n","        # Imprimir el error de validación en verde\n","        print(f\"  {COLOR_GREEN}{error_metric_name} (en Conjunto de Validación Omega_C): {chosen_error_value_val:.10e}{COLOR_RESET}\")\n","\n","        print(f\"  Evaluaciones totales de vectores (función original): {chosen_evaluations_count}\")\n","        print(f\"  Pares (índice, valor) recolectados originalmente: {len(chosen_evaluations_list)}\")\n","        print(f\"    (Número de índices recolectados: {len(chosen_indices_list)})\")\n","        print(f\"    (Tamaño del conjunto de entrenamiento procesado: {Omega_processed_train.shape[0] if hasattr(Omega_processed_train, 'shape') else 0})\")\n","\n","\n","        min_print_count = min(5, len(chosen_indices_list)) # Mostrar de la lista original recolectada\n","        if min_print_count > 0:\n","            print(f\"\\n  Primeros {min_print_count} pares (índice, valor) recolectados originalmente (ejemplo):\")\n","            for i in range(min_print_count):\n","                idx = chosen_indices_list[i] # De la lista original\n","                val = chosen_evaluations_list[i] # De la lista original\n","                # Formatear el índice para impresión\n","                idx_list_str = [str(int(j)) for j in idx] if isinstance(idx, (list, np.ndarray, tuple)) else str(idx)\n","                print(f\"    {i}: Índice={', '.join(idx_list_str)}, Evaluación={val:.8f}\")\n","        else:\n","            print(\"\\n  No se recolectaron pares (índice, valor) para la aproximación elegida.\")\n","\n","    else:\n","        print(f\"\\nNo se seleccionó ninguna aproximación TT adecuada. ({selection_details_message})\")\n","        print(f\"  (Rangos buscados de {min_rank_to_try} a {max_rank_to_try}. Límite de evaluaciones para selección auto: {sizeOmega})\")\n","\n","    print(f\"{'-'*40}\\nFin del barrido de rangos TT-Cross.\\n{'-'*40}\")\n","\n","    return (chosen_tt_approx,\n","            chosen_error_value_val, # Error en el conjunto de validación para el TT elegido\n","            # chosen_evaluations_list, # Lista original de evaluaciones recolectadas\n","            # chosen_indices_list,     # Lista original de índices recolectados\n","            chosen_rank_found,\n","            chosen_evaluations_count,\n","            Omega_processed_train,   # Nuevo: conjunto de entrenamiento procesado (índices)\n","            A_Omega_processed_train) # Nuevo: conjunto de entrenamiento procesado (valores)"],"metadata":{"id":"sHbtlUmOg9Nt","executionInfo":{"status":"ok","timestamp":1749557141104,"user_tz":-120,"elapsed":425,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Optimizadores"],"metadata":{"id":"2MgUcM1xhAHF"}},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import t3f\n","\n","def optimize_tt_with_adam(X, A_Omega, Omega, A_Omega_C, Omega_C, d, number_nodes,\n","                          max_iters=7500, abs_loss_threshold=1e-6, improvement_threshold=1e-3,\n","                          patience=1000, # Patience for training loss stopping criterion\n","                          learning_rate_initial=0.01, decay_steps=400, decay_rate=0.7,\n","                          # Parámetros para ReduceLROnPlateau\n","                          reduce_lr_on_plateau=True, lr_patience=200, lr_factor=0.5,\n","                          lr_min_delta=1e-4, lr_min=1e-6, lr_monitor_interval=100,\n","                          reduce_lr_train_set=False, # NUEVO PARAMETRO: Monitorear entrenamiento para reducir LR\n","                          verbose=False, loss='relative_l2'):\n","    \"\"\"\n","    Optimiza un tensor Train (TT) para completar las entradas faltantes utilizando el optimizador Adam.\n","    Incorpora Decaimiento Exponencial o Reducción de Tasa de Aprendizaje en Plateau.\n","\n","    Args:\n","        X: Tensor TT inicial para la estimación.\n","        A_Omega: Tensor con los valores observados en las posiciones Omega (entrenamiento).\n","        Omega: Índices de las entradas observadas (entrenamiento).\n","        A_Omega_C: Tensor con los valores observados en las posiciones Omega_C (validación).\n","        Omega_C: Índices de las entradas observadas (validación).\n","        max_iters: Número máximo de iteraciones de optimización.\n","        abs_loss_threshold: Umbral absoluto para detener la optimización basado en la pérdida de entrenamiento (ahora RMSE si loss='mse').\n","        improvement_threshold: Umbral de mejora relativa para detener la optimización (entrenamiento, ahora RMSE si loss='mse').\n","        patience: Número de iteraciones a esperar para verificar la mejora en entrenamiento.\n","        learning_rate_initial: Tasa de aprendizaje inicial para el optimizador Adam.\n","        decay_steps: Número de pasos tras los cuales se aplica el decaimiento (si no se usa plateau).\n","        decay_rate: Factor de decaimiento (si no se usa plateau).\n","        reduce_lr_on_plateau: Si es True, usa ReduceLROnPlateau en lugar de decaimiento exponencial fijo.\n","        lr_patience: Número de iteraciones sin mejora en la métrica monitoreada para reducir la tasa de aprendizaje.\n","        lr_factor: Factor por el que se reduce la tasa de aprendizaje (nueva_lr = lr_actual * lr_factor).\n","        lr_min_delta: Umbral mínimo para considerar una mejoría en la métrica monitoreada.\n","        lr_min: Tasa de aprendizaje mínima.\n","        lr_monitor_interval: Cada cuántas iteraciones monitorear la métrica para ReduceLROnPlateau.\n","        reduce_lr_train_set (bool): Si True, monitorea la pérdida de entrenamiento para reducir LR.\n","                                     Si False, monitorea la pérdida de validación.\n","        verbose: Si es True, imprime información sobre el progreso de la optimización.\n","        loss: Tipo de función de pérdida ('mse' o 'relative_l2').\n","\n","    Returns:\n","        Un tuple que contiene:\n","            - estimated: El tensor TT estimado después de la optimización.\n","            - loss_hist: Una lista con el historial de valores de la función de pérdida de entrenamiento (RMSE si loss='mse').\n","            - val_loss_hist: Una lista con el historial de valores de la función de pérdida de validación (RMSE si loss='mse').\n","    \"\"\"\n","    estimated = t3f.get_variable('estimated', initializer=X)\n","\n","    # Configurar el schedule de tasa de aprendizaje\n","    if reduce_lr_on_plateau:\n","        lr_variable = tf.Variable(learning_rate_initial, dtype=tf.float32)\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_initial)\n","        optimizer.learning_rate = lr_variable # Asignamos la Variable al atributo learning_rate del optimizador\n","    else:\n","        lr_schedule_obj = tf.keras.optimizers.schedules.ExponentialDecay(\n","            initial_learning_rate=learning_rate_initial,\n","            decay_steps=decay_steps,\n","            decay_rate=decay_rate,\n","            staircase=False\n","        )\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule_obj)\n","\n","    def calculate_loss(estimated_tensor, values, indices, metric_type='current_loss'):\n","        \"\"\"\n","        Calcula diferentes tipos de pérdida o métricas.\n","        \"\"\"\n","        estimated_vals = t3f.gather_nd(estimated_tensor, indices)\n","        diff = estimated_vals - values\n","\n","        if metric_type == 'mse':\n","            return tf.reduce_mean(diff ** 2)\n","        elif metric_type == 'rmse':\n","            return tf.sqrt(tf.reduce_mean(diff ** 2))\n","        elif metric_type == 'relative_l2':\n","            norm_values = tf.norm(values)\n","            if tf.abs(norm_values) < tf.keras.backend.epsilon():\n","                return tf.norm(diff)\n","            return tf.norm(diff) / norm_values\n","        elif metric_type == 'current_loss':\n","            if loss.lower() == 'mse':\n","                return tf.reduce_mean(diff ** 2)\n","            elif loss.lower() == 'relative_l2':\n","                norm_values = tf.norm(values)\n","                if tf.abs(norm_values) < tf.keras.backend.epsilon():\n","                    return tf.norm(diff)\n","                return tf.norm(diff) / norm_values\n","            else:\n","                raise ValueError(f\"Tipo de pérdida no válido: '{loss}'. Debe ser 'MSE' o 'relative_L2'.\")\n","        else:\n","            raise ValueError(f\"Tipo de métrica no válido: '{metric_type}'.\")\n","\n","    @tf.function\n","    def step_and_update(): # Renombrada para mayor claridad\n","        with tf.GradientTape() as tape:\n","            loss_train_for_grad = calculate_loss(estimated, A_Omega, Omega, metric_type='current_loss')\n","\n","        gradients = tape.gradient(loss_train_for_grad, estimated.tt_cores)\n","        if any(g is None for g in gradients):\n","            tf.print(\"Warning: Found None gradients. Skipping gradient application.\")\n","            # Si hay None gradients, devolvemos las pérdidas actuales para que el bucle continúe si se desea.\n","            # Estas pérdidas son para display y criterios de parada.\n","            loss_train_display = calculate_loss(estimated, A_Omega, Omega, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss')\n","            loss_val_display = calculate_loss(estimated, A_Omega_C, Omega_C, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss')\n","            return loss_train_display, loss_val_display\n","\n","        optimizer.apply_gradients(zip(gradients, estimated.tt_cores))\n","\n","        # Calcular pérdida de validación y entrenamiento para display y criterios de parada.\n","        # Estas pérdidas ya son POST-ACTUALIZACIÓN\n","        loss_train_display = calculate_loss(estimated, A_Omega, Omega, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss')\n","        loss_val_display = calculate_loss(estimated, A_Omega_C, Omega_C, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss')\n","\n","        return loss_train_display, loss_val_display\n","\n","    loss_hist = []\n","    val_loss_hist = []\n","\n","    # Para ReduceLROnPlateau\n","    best_monitor_loss = float('inf')\n","    lr_wait = 0\n","    monitor_set_name = \"Train\" if reduce_lr_train_set else \"Val\"\n","    loss_metric_name = \"RMSE\" if loss.lower() == 'mse' else loss.upper()\n","\n","    sizeOmega = tf.shape(Omega)[0]\n","    sizeOmega_C = tf.shape(Omega_C)[0]\n","\n","    tt_rank = t3f.tt_ranks(X)\n","\n","    print(f\"Starting Adam Tensor Completion for a target tensor.\")\n","    print(f\"Dimensions: {d}, Nodes per dimension: {number_nodes}, Total size: {number_nodes**d}\")\n","    print(f\"Initial ranks: {tt_rank}\")\n","    print(f\"Training points: {sizeOmega}, Validation points: {sizeOmega_C}\")\n","    print(f\"Adam LR Initial: {learning_rate_initial:.4f}, Max Iters: {max_iters}\")\n","    if reduce_lr_on_plateau:\n","        monitor_set = \"Train\" if reduce_lr_train_set else \"Validation\"\n","        print(f\"Using ReduceLROnPlateau: Monitor={monitor_set} {loss_metric_name}, Patience={lr_patience}, Factor={lr_factor:.2f}, MinDelta={lr_min_delta:.1e}, MinLR={lr_min:.1e}, MonitorInterval={lr_monitor_interval}\")\n","    else:\n","        print(f\"Using Exponential Decay: DecaySteps={decay_steps}, DecayRate={decay_rate}\")\n","\n","    print(\"-\" * 30)\n","\n","    # --- Cálculo de la pérdida inicial (antes de la primera actualización) ---\n","    initial_loss_train_v = calculate_loss(estimated, A_Omega, Omega, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss').numpy()\n","    initial_loss_val_v = calculate_loss(estimated, A_Omega_C, Omega_C, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss').numpy()\n","\n","    # Añadir al historial y mostrar el estado inicial\n","    loss_hist.append(initial_loss_train_v)\n","    val_loss_hist.append(initial_loss_val_v)\n","\n","    if verbose:\n","        print(f\"Estado Inicial (Iteración -1): Loss (Train - {loss_metric_name}) = {initial_loss_train_v:.6f}, \"\n","              f\"Loss (Val - {loss_metric_name}) = {initial_loss_val_v:.6f}, LR = {learning_rate_initial:.4f}\")\n","    # Fin del cálculo y muestra de la pérdida inicial\n","\n","    # El resto del código permanece igual, pero el bucle empezará a partir de la Iteración 0\n","    # Es importante que el `best_monitor_loss` para el RLROP se inicialice con la `initial_loss_val_v` o `initial_loss_train_v`\n","    # según lo que se esté monitoreando.\n","    best_monitor_loss = initial_loss_train_v if reduce_lr_train_set else initial_loss_val_v\n","\n","    for i in range(max_iters):\n","        try:\n","            # loss_train_v y loss_val_v ahora siempre contienen la métrica para display (RMSE o relative_l2)\n","            # Y estas pérdidas son POST-ACTUALIZACIÓN de los cores para la iteración `i`\n","            loss_train_v, loss_val_v = step_and_update() # Renombrada para mayor claridad\n","\n","            loss_hist.append(loss_train_v)\n","            val_loss_hist.append(loss_val_v)\n","\n","            # Seleccionar la pérdida a monitorear para RLROP\n","            monitor_loss = loss_train_v if reduce_lr_train_set else loss_val_v\n","\n","        except Exception as e:\n","            print(f\"Error en la iteración {i}: {e}\")\n","            break\n","\n","        if reduce_lr_on_plateau:\n","            current_lr_value = optimizer.learning_rate.numpy()\n","        else:\n","            current_lr_value = optimizer.learning_rate(tf.cast(i + 1, tf.int64)).numpy() # `i + 1` porque el schedule se actualiza por pasos\n","\n","        if verbose and (i + 1) % lr_monitor_interval == 0: # Imprimir en el paso `i+1` (100, 200, etc.)\n","            print(f\"Iteración {i+1}: Loss (Train - {loss_metric_name}) = {loss_train_v:.6f}, \"\n","                  f\"Loss (Val - {loss_metric_name}) = {loss_val_v:.6f}, LR = {current_lr_value:.4f}\")\n","\n","        # --- Lógica de ReduceLROnPlateau ---\n","        # Si la iteración es 0, no aplicamos la lógica de paciencia en el primer check (ya inicializamos best_monitor_loss)\n","        # Empezamos a verificar la paciencia desde el primer intervalo de monitoreo.\n","        if reduce_lr_on_plateau and (i + 1) % lr_monitor_interval == 0 and i > 0: # No monitorear en la Iteración 0\n","            if monitor_loss < best_monitor_loss - lr_min_delta:\n","                best_monitor_loss = monitor_loss\n","                lr_wait = 0\n","                if verbose and (i + 1) % (lr_monitor_interval * 10) == 0:\n","                    print(f\"Iteración {i+1}: Mejora en pérdida de {monitor_set_name} ({monitor_loss:.6f}). Reiniciando paciencia. Best_{monitor_set_name}_loss: {best_monitor_loss:.6f}\")\n","            else:\n","                lr_wait += lr_monitor_interval\n","                if verbose and (i + 1) % (lr_monitor_interval * 10) == 0:\n","                    print(f\"Iteración {i+1}: Sin mejora en pérdida de {monitor_set_name} ({monitor_loss:.6f}). Paciencia: {lr_wait}/{lr_patience}. Best_{monitor_set_name}_loss: {best_monitor_loss:.6f}\")\n","\n","                if lr_wait >= lr_patience:\n","                    old_lr = optimizer.learning_rate.numpy()\n","                    new_lr = max(old_lr * lr_factor, lr_min)\n","                    if new_lr < old_lr - 1e-10:\n","                        optimizer.learning_rate.assign(new_lr)\n","                        if verbose:\n","                            print(f\"Iteración {i+1}: Paciencia agotada ({lr_patience}). Reduciendo LR de {old_lr:.4f} a {new_lr:.4f}.\")\n","                        lr_wait = 0\n","\n","        # --- Criterios de parada basados en la pérdida de entrenamiento ---\n","        # Ajustar el índice para la pérdida en el historial\n","        if i >= patience and (i % patience == 0 or i == max_iters - 1):\n","            if len(loss_hist) > patience:\n","                # Utilizamos `i+1` para el historial porque hemos añadido una entrada inicial\n","                # Por lo tanto, `loss_hist[0]` es la pérdida inicial, `loss_hist[1]` es la de la iteración 0, etc.\n","                # La pérdida `patience` iteraciones atrás será `loss_hist[i+1 - patience]`\n","                loss_prev = loss_hist[i + 1 - patience]\n","                improvement = (loss_prev - loss_train_v) / (loss_prev + tf.keras.backend.epsilon())\n","\n","                if improvement < improvement_threshold:\n","                    if verbose:\n","                        print(f\"Parando en iteración {i+1}: Mejora (entrenamiento {loss_metric_name}: {improvement:.6f}) < {improvement_threshold:.6f} durante {patience} iteraciones.\")\n","                    break\n","\n","        if loss_train_v < abs_loss_threshold:\n","            if verbose:\n","                print(f\"Parando en iteración {i+1}: Loss (entrenamiento {loss_metric_name}) = {loss_train_v:.6e} por debajo del umbral absoluto ({abs_loss_threshold}).\")\n","            break\n","\n","    # --- Resumen Final de la Optimización ---\n","    if verbose:\n","        print(\"\\n--- Resumen Final ---\")\n","        if loss_hist:\n","            print(f\"Pérdida final de entrenamiento ({loss_metric_name}): {loss_hist[-1]:.6f}\")\n","            print(f\"Pérdida final de validación ({loss_metric_name}): {val_loss_hist[-1]:.6f}\")\n","\n","            if loss.lower() == 'relative_l2':\n","                final_rmse_train = calculate_loss(estimated, A_Omega, Omega, metric_type='rmse').numpy()\n","                final_rmse_val = calculate_loss(estimated, A_Omega_C, Omega_C, metric_type='rmse').numpy()\n","                print(f\"Pérdida final de entrenamiento (RMSE): {final_rmse_train:.6f}\")\n","                print(f\"Pérdida final de validación (RMSE): {final_rmse_val:.6f}\")\n","        else:\n","            print(\"No se registraron pérdidas (posiblemente debido a un error temprano).\")\n","\n","    return estimated, loss_hist, val_loss_hist\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import t3f\n","import numpy as np\n","\n","def optimize_tt_with_lbfgs(\n","        X, A_Omega, Omega, A_Omega_C, Omega_C,\n","        d_param, number_nodes,\n","        max_iters=7500, verbose=False, loss='relative_l2',\n","        print_every=25):\n","\n","    # ---------- Inicialización ----------\n","    estimated = t3f.get_variable('estimated', initializer=X)\n","    tt_cores_shapes = [core.shape for core in estimated.tt_cores]\n","    tt_ranks = t3f.tt_ranks(X).numpy()\n","    tensor_shape = estimated.get_shape()\n","    initial_params = tf.concat(\n","        [tf.cast(tf.reshape(c, [-1]), tf.float64) for c in estimated.tt_cores],\n","        axis=0)\n","\n","    # Listas para almacenar las pérdidas por iteración\n","    train_losses_history = []\n","    val_losses_history = []\n","\n","    if verbose:\n","        print(\"\\n========== Comienzo optimización L-BFGS ==========\")\n","        print(\"Formas núcleos TT :\", [s.as_list() for s in tt_cores_shapes])\n","        print(\"Rangos TT         :\", tt_ranks)\n","        print(\"Forma del tensor  :\", tensor_shape.as_list())\n","        print(f\"Parámetros totales: {initial_params.shape[0]}\")\n","        print(\"---------------------------------------------------\")\n","        print(f\"{'Iter':>7} | {'Loss train':>14} | {'Loss val':>14}\")\n","        print(\"---------------------------------------------------\")\n","\n","    # --------- Utilidades internas ---------\n","    def reconstruct_tt(params):\n","        offset, new_cores = 0, []\n","        for shape in tt_cores_shapes:\n","            size = np.prod(shape.as_list())\n","            core = tf.reshape(params[offset:offset+size], shape)\n","            new_cores.append(core)\n","            offset += size\n","        return t3f.TensorTrain(new_cores, shape=tensor_shape, tt_ranks=tt_ranks)\n","\n","    def compute_loss(tt_tensor, indices, values):\n","        \"\"\"Calcula la pérdida entre tt_tensor[indices] y values.\"\"\"\n","        dif = t3f.gather_nd(tt_tensor, indices) - values\n","\n","        if loss == 'relative_l2':\n","            eps = tf.constant(1e-12, dtype=values.dtype)\n","            return tf.norm(dif) / (tf.norm(values) + eps)\n","        # -------- 'mse' por defecto --------\n","        return tf.reduce_mean(tf.square(dif))\n","\n","    # --------------- value + grad ---------------\n","    iter_counter = 0\n","\n","    def loss_and_grad(params):\n","        nonlocal iter_counter\n","        nonlocal train_losses_history, val_losses_history # Acceder a las listas de histórico\n","\n","        with tf.GradientTape() as tape:\n","            tape.watch(params)\n","            temp_est = reconstruct_tt(params)\n","            loss_value = compute_loss(temp_est, Omega, A_Omega)  # train-loss\n","        grads = tape.gradient(loss_value, params)\n","\n","        # ---------- Logs progresivos y almacenamiento ----------\n","        iter_counter += 1\n","\n","        # Calcular y guardar la pérdida de validación en cada step\n","        val_loss_now = compute_loss(temp_est, Omega_C, A_Omega_C).numpy()\n","        train_losses_history.append(loss_value.numpy())\n","        val_losses_history.append(val_loss_now)\n","\n","        if verbose and (iter_counter == 1 or iter_counter % print_every == 0):\n","            print(f\"{iter_counter:7d} | {loss_value.numpy():14.6e} | {val_loss_now:14.6e}\")\n","\n","        return loss_value, grads\n","\n","    # --------------- L-BFGS ---------------\n","    result = tfp.optimizer.lbfgs_minimize(\n","        value_and_gradients_function=loss_and_grad,\n","        initial_position=initial_params,\n","        max_iterations=max_iters)\n","\n","    final_params = result.position\n","    estimated = reconstruct_tt(final_params)\n","\n","    # --------------- Métricas finales ---------------\n","    if verbose:\n","        print(\"\\n--- Resumen L-BFGS ---\")\n","        print(\"Iteraciones realizadas :\", result.num_iterations.numpy())\n","        print(f\"Loss train (final)     : {train_losses_history[-1]:.6f}\") # Usar el último valor del historial\n","        print(f\"Loss val (final)       : {val_losses_history[-1]:.6f}\")   # Usar el último valor del historial\n","        print(\"Convergió?             :\", bool(result.converged.numpy()))\n","        print(\"---------------------------------------------\")\n","\n","    # --- Salidas solicitadas ---\n","    return estimated, np.array(train_losses_history), np.array(val_losses_history), \\\n","           bool(result.converged.numpy())\n","\n","import tensorflow as tf\n","import t3f\n","import functools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from ttml.tensor_train import TensorTrain\n","\n","def linearized_search(A_Omega, X_k, eta_k, Omega):\n","    X_k_proyectado = t3f.gather_nd(X_k, Omega)\n","    eta_k_proyectado = t3f.gather_nd(eta_k, Omega)\n","\n","    A_Omega = tf.constant(A_Omega, dtype=tf.float64)\n","    resta = A_Omega - X_k_proyectado\n","\n","    # Calcular el numerador y el denominador\n","    num = tf.tensordot(eta_k_proyectado, resta, axes=1)\n","    den = tf.tensordot(eta_k_proyectado, eta_k_proyectado, axes=1)\n","\n","    size_A_Omega = tf.size(A_Omega)\n","    size_A_Omega_float = tf.cast(size_A_Omega, tf.float64)\n","\n","    alpha_k =  num / den\n","    return alpha_k\n","\n","def increase_tt_rank_mu(tt_tensor, mu, noise_magnitude=1e-8):\n","    \"\"\"\n","    Incrementa el rango TT en la posición mu+1 siguiendo el algoritmo (4.7).\n","\n","    Args:\n","        tt_tensor (t3f.TensorTrain): Tensor en formato TT.\n","        mu (int): Índice donde se incrementará el rango. Únicos valores posibles: mu = 1,...,d-1\n","        noise_magnitude(float, opcional): Magnitud de los vectores aleatorios R_mu y R_{mu+1}. Default = 1e-8.\n","\n","    Returns:\n","        t3f.TensorTrain: Tensor TT con rango aumentado en la posición mu+1.\n","    \"\"\"\n","    # Paso 1: Ortogonalizar desde la izquierda hasta la posición mu\n","    tt_ortho = t3f.orthogonalize_tt_cores(tt_tensor, left_to_right=True)\n","\n","    # Paso 2: Extraer núcleos TT\n","    tt_cores = tt_ortho.tt_cores  # Lista de núcleos TT. Esto es un tf.Tensor, por eso podemos usar .tt_cores\n","    U_L = tt_cores[mu-1]            # Núcleo en la posición mu\n","    U_R = tt_cores[mu]          # Núcleo en la posición mu+1\n","\n","    # Paso 3: Extraer el vector de rangos TT correctamente\n","    tt_ranks = t3f.tt_ranks(tt_tensor)\n","    r_mu_minus_1 = tt_ranks[mu-1]  # Rango TT en la posición mu-1\n","    r_mu = tt_ranks[mu]            # Rango TT en la posición mu\n","    r_mu_plus_1 = tt_ranks[mu+1]   # Rango TT en la posición mu+1\n","\n","    # Extraer las dimensiones de los modos\n","    n_mu = tt_tensor.get_shape()[mu-1]       # Dimensión en la posición mu\n","    n_mu_plus_1 = tt_tensor.get_shape()[mu]  # Dimensión en la posición mu+1\n","\n","    # Paso 4: Crear vectores aleatorios con la norma deseada\n","    R_mu = tf.random.normal(shape=[r_mu_minus_1 * n_mu, 1], mean=0.0, stddev=noise_magnitude)\n","    R_mu_plus_1 = tf.random.normal(shape=[1, r_mu_plus_1 * n_mu_plus_1], mean=0.0, stddev=noise_magnitude)\n","\n","    # Paso 5: Redimensionar los vectores para que sean compatibles con la estructura TT\n","    R_mu = tf.reshape(R_mu, [r_mu_minus_1, n_mu, 1])  # Convertir a (r_{mu-1}, n_mu, 1)\n","    R_mu_plus_1 = tf.reshape(R_mu_plus_1, [1, n_mu_plus_1, r_mu_plus_1])  # Convertir a (1, n_{mu+1}, r_{mu+1})\n","\n","    # Paso 6: Modificar los núcleos TT en las posiciones mu y mu+1\n","    U_L_new = tf.concat([U_L, R_mu], axis=2)  # Expandir en la tercera dimensión (r_mu+1)\n","    U_R_new = tf.concat([U_R, R_mu_plus_1], axis=0)  # Expandir en la primera dimensión (r_mu+1)\n","\n","    # Paso 7: Reconstruir la lista de núcleos con las modificaciones\n","    new_cores = tt_cores[:mu-1] + (U_L_new, U_R_new) + tt_cores[mu+1:]\n","\n","    # Mostrar la lista de cores y sus dimensiones (Opcional)\n","    # print(\"Lista de new_cores y sus dimensiones:\")\n","    # for i, core in enumerate(new_cores):\n","        # print(f\"Core {i}: Shape {core.shape}\")\n","\n","    # Paso 8: Crear el nuevo tensor TT con los núcleos modificados\n","    tt_tensor_updated = t3f.TensorTrain(new_cores)\n","\n","    return tt_tensor_updated\n","\n","def truncate(tensor, target_ranks):\n","    \"\"\"Trunca un tensor TT a los rangos objetivo.\"\"\"\n","    #return t3f.round(tensor, max_tt_rank=target_ranks, epsilon=1e-6)\n","    return t3f.round(tensor, max_tt_rank=target_ranks) # He cambiado esto, comprobar si epsilon=None funciona para nuestros propósitos\n","\n","# Gradiente Riemanniano automático\n","def calcular_gradiente_riemanniano_tf(X, A_Omega, Omega):\n","    X_ortho = t3f.orthogonalize_tt_cores(X, left_to_right=True)\n","    funcion_objetivo_parcial = functools.partial(funcion_objetivo, A_Omega=A_Omega, Omega=Omega)\n","    gradiente = t3f.gradients(funcion_objetivo_parcial, X_ortho, runtime_check=False)\n","    return gradiente # El resultado es un TensorTrain de t3f\n","\n","def funcion_objetivo(X, A_Omega, Omega):\n","    X_Omega = t3f.gather_nd(X, Omega)  # Usar t3f.gather_nd para TensorTrain\n","    A_Omega = tf.constant(A_Omega, dtype=tf.float64)\n","    size_A_Omega = tf.size(A_Omega)\n","    size_A_Omega_float = tf.cast(size_A_Omega, tf.float64)\n","    Z = X_Omega - A_Omega\n","\n","    # Calcular el producto escalar de Z consigo mismo\n","    producto_escalar_Z = tf.tensordot(Z, Z, axes=1)\n","\n","    return 0.5 * producto_escalar_Z / size_A_Omega_float\n","    #return 0.5 * producto_escalar_Z\n","\n","def riemannian_tensor_completion(X_0, A_Omega, Omega, A_Omega_C, Omega_C, d, number_nodes,\n","                                 max_iters=100, tol_riemannian_tensor_completion=1e-4,\n","                                 loss='relative_l2', verbose=False, print_interval=100):\n","    \"\"\"\n","    Realiza la compleción de tensores utilizando un método de gradiente conjugado Riemanniano.\n","\n","    Args:\n","        X_0: Tensor TT inicial para la estimación.\n","        A_Omega: Tensor con los valores observados (entrenamiento).\n","        Omega: Índices de las entradas observadas (entrenamiento).\n","        A_Omega_C: Tensor con los valores observados (validación).\n","        Omega_C: Índices de las entradas observadas (validación).\n","        d: Número de dimensiones del tensor.\n","        number_nodes: Número de nodos por dimensión.\n","        max_iters: Número máximo de iteraciones.\n","        tol_riemannian_tensor_completion: Tolerancia para el criterio de parada.\n","        loss: Tipo de función de pérdida ('mse' o 'relative_l2').\n","        verbose: Si es True, imprime información sobre el progreso.\n","        print_interval: Cada cuántas iteraciones imprimir el estado.\n","\n","    Returns:\n","        Un tuple que contiene:\n","            - X_k: El tensor TT estimado después de la optimización.\n","            - loss_hist: Lista con el historial de la pérdida de entrenamiento.\n","            - val_loss_hist: Lista con el historial de la pérdida de validación.\n","    \"\"\"\n","    def calculate_loss(estimated_tensor, values, indices, metric_type='current_loss'):\n","        \"\"\"\n","        Calcula diferentes tipos de pérdida o métricas.\n","        \"\"\"\n","        estimated_vals = t3f.gather_nd(estimated_tensor, indices)\n","        diff = estimated_vals - values\n","\n","        if metric_type == 'mse':\n","            return tf.reduce_mean(diff ** 2)\n","        elif metric_type == 'rmse':\n","            return tf.sqrt(tf.reduce_mean(diff ** 2))\n","        elif metric_type == 'relative_l2':\n","            norm_values = tf.norm(values)\n","            if tf.abs(norm_values) < tf.keras.backend.epsilon():\n","                return tf.norm(diff)\n","            return tf.norm(diff) / norm_values\n","        elif metric_type == 'current_loss':\n","            if loss.lower() == 'mse':\n","                return tf.reduce_mean(diff ** 2)\n","            elif loss.lower() == 'relative_l2':\n","                norm_values = tf.norm(values)\n","                if tf.abs(norm_values) < tf.keras.backend.epsilon():\n","                    return tf.norm(diff)\n","                return tf.norm(diff) / norm_values\n","            else:\n","                raise ValueError(f\"Tipo de pérdida no válido: '{loss}'.\")\n","        else:\n","            raise ValueError(f\"Tipo de métrica no válido: '{metric_type}'.\")\n","\n","    # --- Inicialización ---\n","    X_k = X_0\n","    target_ranks = t3f.tt_ranks(X_0).numpy()\n","    loss_hist = []\n","    val_loss_hist = []\n","    loss_metric_name = \"RMSE\" if loss.lower() == 'mse' else loss.upper()\n","\n","    # --- Informes Iniciales ---\n","    sizeOmega = tf.shape(Omega)[0]\n","    sizeOmega_C = tf.shape(Omega_C)[0]\n","    print(\"Starting Riemannian Tensor Completion for a target tensor.\")\n","    print(f\"Dimensions: {d}, Nodes per dimension: {number_nodes}, Total size: {number_nodes**d}\")\n","    print(f\"Initial ranks: {target_ranks}\")\n","    print(f\"Training points: {sizeOmega}, Validation points: {sizeOmega_C}\")\n","    print(f\"Max Iters: {max_iters}, Tolerance: {tol_riemannian_tensor_completion:.1e}\")\n","    print(\"-\" * 30)\n","\n","    # --- Cálculo de la pérdida inicial ---\n","    initial_loss_train = calculate_loss(X_k, A_Omega, Omega, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss').numpy()\n","    initial_loss_val = calculate_loss(X_k, A_Omega_C, Omega_C, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss').numpy()\n","    loss_hist.append(initial_loss_train)\n","    val_loss_hist.append(initial_loss_val)\n","\n","    if verbose:\n","        print(f\"Estado Inicial (Iteración 0): Loss (Train - {loss_metric_name}) = {initial_loss_train:.6f}, \"\n","              f\"Loss (Val - {loss_metric_name}) = {initial_loss_val:.6f}\")\n","\n","    # --- Primera iteración (fuera del bucle principal para inicializar valores) ---\n","    xi_k = calcular_gradiente_riemanniano_tf(X_k, A_Omega, Omega)\n","    eta_k = -xi_k\n","    alpha_k = linearized_search(A_Omega, X_k, eta_k, Omega)\n","    X_temp = X_k + alpha_k * eta_k\n","    X_k_new = truncate(X_temp, target_ranks)\n","\n","    # Guardar estado para la siguiente iteración\n","    ip_xi_xi_old = t3f.frobenius_norm_squared(xi_k)\n","    eta_k_anterior = eta_k\n","    X_k = X_k_new\n","\n","    # --- Bucle de Optimización Principal ---\n","    for k in range(1, max_iters + 1):\n","        # Calcular gradiente y dirección conjugada\n","        xi_k = calcular_gradiente_riemanniano_tf(X_k, A_Omega, Omega)\n","        eta_transported = t3f.project(eta_k_anterior, X_k)\n","\n","        ip_xi_xi = t3f.frobenius_norm_squared(xi_k)\n","        beta_k = ip_xi_xi / ip_xi_xi_old if ip_xi_xi_old != 0 else 0\n","        eta_k = -xi_k + beta_k * eta_transported\n","\n","        # Búsqueda de paso y actualización del tensor\n","        alpha_k = linearized_search(A_Omega, X_k, eta_k, Omega)\n","        X_temp = X_k + alpha_k * eta_k\n","        X_k_new = truncate(X_temp, target_ranks)\n","\n","        # Calcular y almacenar las pérdidas de la iteración actual\n","        loss_train_v = calculate_loss(X_k_new, A_Omega, Omega, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss').numpy()\n","        loss_val_v = calculate_loss(X_k_new, A_Omega_C, Omega_C, metric_type='rmse' if loss.lower() == 'mse' else 'current_loss').numpy()\n","        loss_hist.append(loss_train_v)\n","        val_loss_hist.append(loss_val_v)\n","\n","        # Informar del progreso\n","        if verbose and k % print_interval == 0:\n","            print(f\"Iteración {k}: Loss (Train - {loss_metric_name}) = {loss_train_v:.6f}, \"\n","                  f\"Loss (Val - {loss_metric_name}) = {loss_val_v:.6f}\")\n","\n","        # Criterio de parada (basado en la diferencia relativa de los tensores)\n","        diff_norm = t3f.frobenius_norm(X_k_new - X_k)\n","        x_norm = t3f.frobenius_norm(X_k)\n","        relative_diff = diff_norm / (x_norm + tf.keras.backend.epsilon())\n","        '''\n","        if relative_diff < tol_riemannian_tensor_completion:\n","            if verbose:\n","                print(f\"\\nParando en iteración {k}: La mejora relativa ({relative_diff:.3e}) es menor que la tolerancia ({tol_riemannian_tensor_completion:.1e}).\")\n","            X_k = X_k_new\n","            break\n","        '''\n","        # Actualizar variables para la siguiente iteración\n","        X_k = X_k_new\n","        eta_k_anterior = eta_k\n","        ip_xi_xi_old = ip_xi_xi\n","\n","        if k == max_iters:\n","            if verbose:\n","                print(f\"\\nParando en iteración {k}: Se alcanzó el número máximo de iteraciones.\")\n","\n","    # --- Resumen Final de la Optimización ---\n","    if verbose:\n","        print(\"\\n--- Resumen Final ---\")\n","        if loss_hist:\n","            final_train_loss = loss_hist[-1]\n","            final_val_loss = val_loss_hist[-1]\n","            print(f\"Pérdida final de entrenamiento ({loss_metric_name}): {final_train_loss:.6f}\")\n","            print(f\"Pérdida final de validación ({loss_metric_name}): {final_val_loss:.6f}\")\n","\n","            # Si la pérdida principal no era RMSE, calcularlo para el informe final\n","            if loss.lower() != 'mse':\n","                final_rmse_train = calculate_loss(X_k, A_Omega, Omega, metric_type='rmse').numpy()\n","                final_rmse_val = calculate_loss(X_k, A_Omega_C, Omega_C, metric_type='rmse').numpy()\n","                print(f\"Pérdida final de entrenamiento (RMSE): {final_rmse_train:.6f}\")\n","                print(f\"Pérdida final de validación (RMSE): {final_rmse_val:.6f}\")\n","        else:\n","            print(\"No se registraron pérdidas.\")\n","\n","    return X_k, loss_hist, val_loss_hist"],"metadata":{"id":"j4H3THXfhBnt","executionInfo":{"status":"ok","timestamp":1749553554667,"user_tz":-120,"elapsed":4434,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Función directora"],"metadata":{"id":"XYQAP2X8hJyU"}},{"cell_type":"code","source":["import numpy as np\n","import t3f # Assuming t3f is installed and provides TensorTrain and other functionalities\n","import matplotlib.pyplot as plt # For plotting\n","\n","# =============================================================================\n","# GLOBAL VARIABLES (for custom_functions)\n","# =============================================================================\n","total_vector_evaluations = 0\n","d = 10 # This global 'd' will be used by the custom functions (f1-f4)\n","# Declaring custom_function globally to be accessible by other parts of the pipeline\n","# This will be assigned within run_tensor_completion_pipeline\n","custom_function = None\n","\n","# =============================================================================\n","# CUSTOM FUNCTIONS (f1 to f4)\n","# =============================================================================\n","\n","def f1(x):\n","    \"\"\"\n","    The actual function being approximated (f1).\n","    Takes a numpy array x of shape (num_samples, d).\n","    Increments total_vector_evaluations by num_samples.\n","    \"\"\"\n","    global total_vector_evaluations\n","    global d\n","\n","    if len(x.shape) == 1:\n","        x_input = x.reshape(1, -1)\n","    else:\n","        x_input = x\n","\n","    num_samples, d_check = x_input.shape\n","    if d_check != d:\n","        raise ValueError(f\"Dimension mismatch in f1: Expected {d}, got {d_check}\")\n","\n","    total_vector_evaluations += num_samples\n","\n","    with np.errstate(divide='ignore', invalid='ignore', over='ignore', under='ignore'):\n","        x0 = x_input[:, 0]\n","        x1 = x_input[:, 1]\n","        x2 = x_input[:, 2]\n","        x3 = x_input[:, 3]\n","        x4 = x_input[:, 4]\n","        x5 = x_input[:, 5]\n","        x6 = x_input[:, 6]\n","        x7 = x_input[:, 7]\n","        x8 = x_input[:, 8]\n","        x9 = x_input[:, 9]\n","\n","        term1 = x0 ** 2\n","        term2 = np.exp(x1)\n","        term3 = np.log(x2 + 1)\n","        term4 = 1 / (x3 + 1)\n","        term5 = x4 ** 3\n","        term6 = np.sqrt(x5)\n","        term7 = x6 * x7\n","        term8 = x8 ** 4\n","        term9 = -np.exp(-x9)\n","\n","        values = term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9\n","    return values\n","\n","def f2(x):\n","    \"\"\"\n","    The actual function being approximated (f2).\n","    Takes a numpy array x of shape (num_samples, d).\n","    Increments total_vector_evaluations by num_samples.\n","    \"\"\"\n","    global total_vector_evaluations\n","    global d\n","\n","    if len(x.shape) == 1:\n","        x_input = x.reshape(1, -1)\n","    else:\n","        x_input = x\n","\n","    num_samples, d_check = x_input.shape\n","    if d_check != d:\n","        raise ValueError(f\"Dimension mismatch in f2: Expected {d}, got {d_check}\")\n","\n","    total_vector_evaluations += num_samples\n","\n","    with np.errstate(divide='ignore', invalid='ignore', over='ignore', under='ignore'):\n","        values = ((x_input[:, 0] ** 2) * (x_input[:, 9] ** 2) * (x_input[:, 8] ** 3) +\n","                  np.exp(-x_input[:, 1] ** 2 + x_input[:, 5] ** 4 - x_input[:, 7] * x_input[:, 6]) +\n","                  np.log(1 + x_input[:, 3] + x_input[:, 2] * x_input[:, 7]) +\n","                  (x_input[:, 0] ** 2 + x_input[:, 1] + x_input[:, 4]) /\n","                  (1 + x_input[:, 5] ** 3 + x_input[:, 2] ** 2 + x_input[:, 8] ** 4))\n","    return values\n","\n","def f3(x):\n","    \"\"\"\n","    The actual function being approximated (f3).\n","    Takes a numpy array x of shape (num_samples, d).\n","    Increments total_vector_evaluations by num_samples.\n","    \"\"\"\n","    global total_vector_evaluations\n","    global d\n","\n","    if len(x.shape) == 1:\n","        x_input = x.reshape(1, -1)\n","    else:\n","        x_input = x\n","\n","    num_samples, d_check = x_input.shape\n","    if d_check != d:\n","        raise ValueError(f\"Dimension mismatch in f3: Expected {d}, got {d_check}\")\n","\n","    total_vector_evaluations += num_samples\n","\n","    with np.errstate(divide='ignore', invalid='ignore', over='ignore', under='ignore'):\n","        term1_base = (x_input[:, 0] ** 2) + 5 * np.sin(4 * np.pi * x_input[:, 0])\n","        term1 = term1_base * (x_input[:, 9] ** 2) * (x_input[:, 8] ** 3)\n","\n","        term2 = np.exp(-x_input[:, 1] ** 2 + x_input[:, 5] ** 4 - x_input[:, 7] * x_input[:, 6])\n","        term3 = np.log(1 + x_input[:, 3] + x_input[:, 2] * x_input[:, 7])\n","\n","        numerator_term4 = x_input[:, 0] ** 2 + x_input[:, 1] + x_input[:, 4]\n","        denominator_term4 = 1 + x_input[:, 5] ** 3 + x_input[:, 2] ** 2 + x_input[:, 8] ** 4\n","        term4 = numerator_term4 / denominator_term4\n","\n","        term_min_x1 = 50 * (x_input[:, 1] - 0.5) ** 2\n","\n","        values = term1 + term2 + term3 + term4 + term_min_x1\n","    return values\n","\n","def f4(x):\n","    \"\"\"\n","    Una nueva función compleja de 10 dimensiones para problemas de compleción tensorial (f4).\n","    Esta función incorpora una variedad de términos no lineales e interactivos\n","    para crear un paisaje funcional rico y diverso.\n","\n","    La función toma un array de numpy 'x' con forma (num_samples, d).\n","    Incrementa la variable global 'total_vector_evaluations' por 'num_samples',\n","    contabilizando el número total de vectores procesados en esta llamada.\n","\n","    Parámetros:\n","    ----------\n","    x : numpy.ndarray\n","        Un array de numpy que representa los puntos de entrada. Puede ser:\n","        - De forma (d,) para una única evaluación vectorial.\n","        - De forma (num_samples, d) para una evaluación por lotes de múltiples vectores.\n","\n","    Retorna:\n","    -------\n","    numpy.ndarray\n","        Un array de numpy con las evaluaciones de la función para cada vector de entrada.\n","        La forma será (num_samples,) o un escalar si la entrada fue (d,).\n","\n","    Levanta:\n","    -------\n","    ValueError\n","        Si la dimensión de entrada (d_check) no coincide con la dimensión esperada 'd'.\n","    \"\"\"\n","    global total_vector_evaluations\n","    global d\n","\n","    # Asegura que la entrada sea bidimensional (num_samples, d) para facilitar el procesamiento por lotes\n","    if len(x.shape) == 1:\n","        x_input = x.reshape(1, -1)\n","    else:\n","        x_input = x\n","\n","    # Verifica que la dimensión de entrada coincida con la dimensión esperada 'd'\n","    num_samples, d_check = x_input.shape\n","    if d_check != d:\n","        raise ValueError(f\"Desajuste de dimensión en complex_multidimensional_function: Esperado {d}, obtenido {d_check}\")\n","\n","    # Incrementa el contador global de evaluaciones de vectores\n","    total_vector_evaluations += num_samples\n","\n","    # Calcula los valores para el lote de entrada, suprimiendo advertencias\n","    # para operaciones como divisiones por cero, logaritmos de cero/negativos, etc.,\n","    # lo cual es útil si los puntos de entrada no están estrictamente dentro del dominio deseado.\n","    with np.errstate(divide='ignore', invalid='ignore', over='ignore', under='ignore'):\n","        # Término 1: Interacción oscilatoria entre x0 y x1\n","        # Multiplica senos y cosenos para crear patrones de onda que dependen de ambas dimensiones.\n","        term1 = 10 * np.sin(2 * np.pi * x_input[:, 0]) * np.cos(3 * np.pi * x_input[:, 1])\n","\n","        # Término 2: Función sigmoide que depende de la suma de x2, x3 y x4\n","        # Esto genera una transición suave (en forma de 'S') en el valor de la función\n","        # a medida que la suma de estas tres dimensiones varía.\n","        term2 = 5 / (1 + np.exp(-(x_input[:, 2] + x_input[:, 3] + x_input[:, 4] - 1.5) * 5))\n","\n","        # Término 3: Pico Gaussiano (similar a una RBF) centrado en (0.5, 0.5, 0.5) para x5, x6, x7\n","        # Este término crea una 'montaña' o 'valle' localizada en el espacio de la función,\n","        # contribuyendo a la complejidad local. La varianza (0.05) controla el ancho del pico.\n","        term3 = 15 * np.exp(-((x_input[:, 5] - 0.5)**2 + (x_input[:, 6] - 0.5)**2 + (x_input[:, 7] - 0.5)**2) / 0.05)\n","\n","        # Término 4: Tangente hiperbólica de un producto polinomial de x8 y x9\n","        # Introduce una no linealidad fuerte y acotada, que es común en funciones de activación de redes neuronales.\n","        term4 = 8 * np.tanh(x_input[:, 8] * x_input[:, 9]**2 - 0.5)\n","\n","        # Término 5: Interacciones polinómicas lineales cruzadas entre dimensiones\n","        # Aporta una contribución lineal de productos de pares de dimensiones.\n","        term5 = (x_input[:, 0] * x_input[:, 5] +\n","                 x_input[:, 1] * x_input[:, 6] +\n","                 x_input[:, 2] * x_input[:, 7] +\n","                 x_input[:, 3] * x_input[:, 8] +\n","                 x_input[:, 4] * x_input[:, 9])\n","\n","        # Combina todos los términos para obtener el valor final de la función\n","        values = term1 + term2 + term3 + term4 + term5\n","\n","        # Reemplaza cualquier NaN (Not a Number) o Inf (Infinito) resultante de operaciones numéricas\n","        # con valores finitos para asegurar la robustez.\n","        values = np.nan_to_num(values, nan=0.0, posinf=1e10, neginf=-1e10)\n","    return values\n","\n","# Map of function names to actual function objects\n","function_map = {\n","    'f1': f1,\n","    'f2': f2,\n","    'f3': f3,\n","    'f4': f4\n","}\n","\n","# =============================================================================\n","# OPTIMIZATION PIPELINE FUNCTION\n","# =============================================================================\n","\n","def run_tensor_completion_pipeline(function_choice, optimization_method,\n","                                 d_param, MODAL_SIZE, sizeOmega_max,\n","                                 min_rank_to_try, max_rank_to_try, manual_rank, seed_tt_cross,\n","                                 tol_flattening, max_its_tt_cross, tol_precision,\n","                                 metric, tt_cross_algorithm_func, dmrg_rank_kick,\n","                                 sizeOmega_C, TEST_SET_SEED, increase_amount, sizeOmegaExtra,\n","                                 verbose=True, graphics=True):\n","    \"\"\"\n","    Executes the tensor completion pipeline, including TT-Cross and an optional optimization method.\n","\n","    Args:\n","        function_choice (str): The name of the function to approximate ('f1', 'f2', 'f3', 'f4').\n","        optimization_method (str or None): The optimization method to use ('adam', 'lbfgs', 'rttc', or None).\n","        d_param (int): Dimension of the tensor.\n","        MODAL_SIZE (int): Desired modal size (number of points per dimension).\n","        sizeOmega_max (int): Maximum allowed total vector evaluations for TT-Cross.\n","        min_rank_to_try (int): Minimum TT rank to try for TT-Cross.\n","        max_rank_to_try (int): Maximum TT rank to try for TT-Cross.\n","        manual_rank (int or None): Manual rank chosen for the TT approximation.\n","        seed_tt_cross (int): Seed for TT-Cross randomness.\n","        tol_flattening (float): Flattening tolerance for TT-Cross.\n","        max_its_tt_cross (int): Maximum number of sweeps for TT-Cross.\n","        tol_precision (float): Precision tolerance (target loss) for TT-Cross.\n","        metric (str): Loss metric ('rmse' or 'relative_l2').\n","        tt_cross_algorithm_func (callable): The TT-Cross algorithm function (e.g., tt_cross_regular_v2).\n","        dmrg_rank_kick (int): DMRG rank kick parameter.\n","        sizeOmega_C (int): Size of the validation set.\n","        TEST_SET_SEED (int): Seed for test set generation.\n","        increase_amount (int): Amount to increase TT rank before optimization.\n","        sizeOmegaExtra (int): Extra points to augment training set.\n","        verbose (bool, optional): Whether to print verbose output. Defaults to True.\n","        graphics (bool, optional): Whether to plot graphics. Defaults to True.\n","\n","    Returns:\n","        tuple: A tuple containing (X_optimized_final, loss_hist_final, val_loss_hist_final).\n","               X_optimized_final: The optimized TT approximation (or best_tt_approx if no optimization).\n","               loss_hist_final: List of training loss history (None if no optimization).\n","               val_loss_hist_final: List of validation loss history (None if no optimization).\n","    \"\"\"\n","    global total_vector_evaluations\n","    global d # Ensure global 'd' is set to the parameter 'd_param' for consistency\n","    global custom_function # Declare custom_function as global to modify it\n","\n","    d = d_param\n","    total_vector_evaluations = 0 # Reset evaluation counter for this pipeline run\n","\n","    # Select the custom_function based on choice and assign it to the global variable\n","    if function_choice not in function_map:\n","        raise ValueError(f\"Invalid function_choice: {function_choice}. Choose from {list(function_map.keys())}\")\n","    custom_function = function_map[function_choice] # Assign to global custom_function\n","\n","    print(f\"\\n--- Running Tensor Completion Pipeline for Function: {function_choice} ---\")\n","\n","    # --- TT-Cross Step ---\n","    print(f\"\\n--- Running TT-Cross for Function {function_choice} ---\")\n","    Omega_C, A_Omega_C = generate_validation_set(\n","        sizeOmega_C=sizeOmega_C, d=d, number_nodes=MODAL_SIZE,\n","        custom_function=custom_function, # Now uses the global custom_function, which is correctly set\n","        seed=TEST_SET_SEED\n","    )\n","\n","    best_tt_approx, chosen_error_value_val, chosen_rank_found, \\\n","    chosen_evaluations_count, Omega, A_Omega = optimize_tt_cross_rank_sweep(\n","        d=d, MODAL_SIZE=MODAL_SIZE, sizeOmega=sizeOmega_max,\n","        min_rank_to_try=min_rank_to_try, max_rank_to_try=max_rank_to_try,\n","        seed=seed_tt_cross, tol_flattening=tol_flattening, max_its=max_its_tt_cross,\n","        tol_precision=tol_precision,\n","        physical_point_index_fun=physical_point_index_fun,\n","        collecting_index_fun=collecting_index_fun,\n","        tt_cross_algorithm_func=tt_cross_algorithm_func,\n","        create_tt_random=create_tt_random, create_tt_initial=create_tt_initial,\n","        get_tt_shape=get_tt_shape, TensorTrain=TensorTrain,\n","        index_function_wrapper=index_function_wrapper,\n","        Omega_C=Omega_C, A_Omega_C=A_Omega_C,\n","        _calculate_tt_approximation_error_value=_calculate_tt_approximation_error_value,\n","        process_and_verify=process_and_verify,\n","        dmrg_rank_kick=dmrg_rank_kick, metric=metric, manual_rank=manual_rank\n","    )\n","\n","    check_approximation_accuracy(best_tt_approx, Omega, A_Omega, Omega_C, A_Omega_C, metric='relative_l2')\n","    print(f\"TT-Cross final validation error for Function {function_choice}: {chosen_error_value_val:.6f}\")\n","    print(f\"Total vector evaluations after TT-Cross: {total_vector_evaluations}\")\n","\n","\n","    # --- Handle No Optimization Case ---\n","    if optimization_method is None:\n","        print(f\"\\nOptimization method set to None. Stopping after TT-Cross for Function {function_choice}.\")\n","        # Return best_tt_approx as the 'optimized' result, with no loss history\n","        return best_tt_approx, None, None\n","\n","    # --- Prepare for Optimization Step ---\n","    best_tt_approx_t3f = process_tt_approximation_and_convert(best_tt_approx, increase_amount)\n","    seed_OmegaExtra = TEST_SET_SEED + 23 # Consistent seed for augmentation\n","    Omega, A_Omega = augment_training_set(Omega, A_Omega, sizeOmegaExtra, MODAL_SIZE, d, custom_function, seed_OmegaExtra) # Uses global custom_function\n","    print(f\"Total vector evaluations after training set augmentation: {total_vector_evaluations}\")\n","\n","    # Determine the suffix based on manual_rank. If manual_rank is None, use '4'.\n","    current_rank_suffix = manual_rank if manual_rank is not None else 4\n","\n","    # Variables to store the results of the chosen optimizer\n","    X_optimized_current = None\n","    loss_hist_current = None\n","    val_loss_hist_current = None\n","    method_title_suffix = \"\"\n","    method_key_prefix = \"\"\n","\n","    # --- Logic for selecting the Optimization Method ---\n","    if optimization_method == 'adam':\n","        print(f\"\\n--- Running with Adam optimizer for Function {function_choice} ---\")\n","        tt_shape = get_tt_shape(best_tt_approx)\n","        d_optimizer = len(tt_shape) # Redefine d based on tt_shape for optimizer\n","\n","        X = best_tt_approx_t3f\n","\n","        # Adam Hyperparameters\n","        learning_rate_initial = 0.000001\n","        decay_steps = 700\n","        decay_rate = 0.7\n","        max_iters_adam = 50000 # Specific max_iters for Adam\n","        abs_loss_threshold = 1e-4\n","        improvement_threshold = 1e-4\n","        patience_adam = 15000\n","        lr_reduce_on_plateau = True\n","        lr_patience = 1000\n","        lr_factor = 0.7\n","        lr_min_delta = 1e-5\n","        lr_min = 1e-6\n","        lr_monitor_interval = 100\n","        reduce_lr_train_set = True\n","\n","        # Execute tensor completion algorithm with Adam\n","        X_optimized_current, loss_hist_current, val_loss_hist_current = optimize_tt_with_adam(\n","            X, A_Omega, Omega, A_Omega_C, Omega_C, d_optimizer, MODAL_SIZE,\n","            max_iters=max_iters_adam,\n","            abs_loss_threshold=abs_loss_threshold,\n","            improvement_threshold=improvement_threshold,\n","            patience=patience_adam,\n","            learning_rate_initial=learning_rate_initial,\n","            decay_steps=decay_steps,\n","            decay_rate=decay_rate,\n","            reduce_lr_on_plateau=lr_reduce_on_plateau,\n","            lr_patience=lr_patience,\n","            lr_factor=lr_factor,\n","            lr_min_delta=lr_min_delta,\n","            lr_min=lr_min,\n","            lr_monitor_interval=lr_monitor_interval,\n","            reduce_lr_train_set=reduce_lr_train_set,\n","            verbose=verbose,\n","            loss=metric # Use global metric for loss\n","        )\n","        method_key_prefix = 'adam'\n","        method_title_suffix = \"Adam Optimization\"\n","\n","    elif optimization_method == 'lbfgs':\n","        print(f\"\\n--- Running with L-BFGS optimizer for Function {function_choice} ---\")\n","        tt_shape = get_tt_shape(best_tt_approx)\n","        d_optimizer = len(tt_shape) # Redefine d based on tt_shape for optimizer\n","\n","        X = best_tt_approx_t3f\n","\n","        max_iters_lbfgs = 2500 # Specific max_iters for L-BFGS\n","        print_every = 50\n","\n","        # Execute tensor completion algorithm with L-BFGS\n","        X_optimized_current, loss_hist_current, \\\n","        val_loss_hist_current, converged_lbfgs = optimize_tt_with_lbfgs(\n","            X, A_Omega, Omega, A_Omega_C, Omega_C,\n","            d_param=d_optimizer, number_nodes=MODAL_SIZE,\n","            max_iters=max_iters_lbfgs, verbose=verbose, loss=metric, # Use global metric for loss\n","            print_every=print_every\n","        )\n","        method_key_prefix = 'lbfgs'\n","        method_title_suffix = \"L-BFGS Optimization\"\n","\n","    elif optimization_method == 'rttc':\n","        print(f\"\\n--- Running with Riemannian Tensor Completion (RTTC) optimizer for Function {function_choice} ---\")\n","        max_iters_rttc = 1000 # Default max_iters for RTTC\n","        tol_rttc = 1e-4 # Specific tolerance for RTTC\n","        print_interval_rttc = 100\n","\n","        # Execute optimization with RTTC\n","        X_optimized_current, loss_hist_current, val_loss_hist_current = riemannian_tensor_completion(\n","            best_tt_approx_t3f, A_Omega, Omega, A_Omega_C, Omega_C, d, MODAL_SIZE,\n","            max_iters=max_iters_rttc, tol_riemannian_tensor_completion=tol_rttc,\n","            loss=metric, verbose=verbose, print_interval=print_interval_rttc # Use global metric for loss\n","        )\n","        method_key_prefix = 'rttc'\n","        method_title_suffix = \"Riemannian Optimization\"\n","\n","    else:\n","        raise ValueError(\"Invalid optimization_method. Choose 'adam', 'lbfgs', 'rttc', or None.\")\n","\n","    # Use the retrieved variables in plotting and verification functions\n","    if graphics and loss_hist_current is not None: # Only plot if loss history exists (i.e., not None optimization)\n","        plot_loss_history(loss_hist_current, val_loss_hist_current, loss_metric_name='Relative L2 Error',\n","                          title=f'{method_title_suffix} (Función {function_choice}, Rango {current_rank_suffix}): Error L2 Relativo vs. Iteración')\n","    elif graphics:\n","        print(\"No loss history to plot for this run (optimization method is None).\")\n","\n","    check_approximation_accuracy(X_optimized_current, Omega, A_Omega, Omega_C, A_Omega_C, metric='relative_l2')\n","    check_approximation_accuracy(X_optimized_current, Omega, A_Omega, Omega_C, A_Omega_C, metric='relative_l1')\n","    print(f\"Total vector evaluations after optimization: {total_vector_evaluations}\")\n","\n","    return X_optimized_current, loss_hist_current, val_loss_hist_current"],"metadata":{"id":"_Iqmc6sThLQd","executionInfo":{"status":"ok","timestamp":1749553576211,"user_tz":-120,"elapsed":51,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Pruebas"],"metadata":{"id":"qKJS0VqVhOnO"}},{"cell_type":"markdown","source":["Antiguo:"],"metadata":{"id":"82O4dHHJvT9P"}},{"cell_type":"code","source":["tol_flattening = 1e-2 # Flattening tolerance (based on percentage change of loss)\n","tol_precision = 5e-3 # Precision tolerance (target loss)"],"metadata":{"id":"yIuQRQXPvUtc","executionInfo":{"status":"ok","timestamp":1749557268691,"user_tz":-120,"elapsed":3,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# =============================================================================\n","# MAIN PARAMETER CHOICE AND PIPELINE EXECUTION\n","# =============================================================================\n","\n","# --- Choose Function and Optimization Method ---\n","function_to_approximate = 'f2' # Choose 'f1', 'f2', 'f3', or 'f4'\n","optimization_method_choice = None # Choose 'adam', 'lbfgs', 'rttc', or None\n","\n","# Main Parameters\n","d = 10 # Dimension of the tensor/function (also sets the global 'd')\n","MODAL_SIZE = 10 # Desired modal size (number of points per dimension)\n","number_nodes = MODAL_SIZE # Alias for MODAL_SIZE\n","\n","sizeOmega_max = 10000 # Maximum allowed total vector evaluations for TT-Cross\n","\n","min_rank_to_try = 1\n","max_rank_to_try = 10\n","manual_rank = 9 # Set manual_rank to an integer (e.g., 5) or None\n","seed_tt_cross = 610022\n","\n","# TT-Cross Hyperparameters\n","tol_flattening = 1e-4 # Flattening tolerance (based on percentage change of loss)\n","tol_precision = 1e-4 # Precision tolerance (target loss)\n","max_its_tt_cross = 20 # Maximum number of sweeps for TT-Cross\n","\n","tol_flattening = 1e-2 # Flattening tolerance (based on percentage change of loss)\n","tol_precision = 5e-3 # Precision tolerance (target loss)\n","\n","\n","metric = 'relative_l2' # 'rmse' or 'relative_l2'\n","\n","tt_cross_algorithm_func = tt_cross_regular_v2 # tt_cross_regular_v2 or tt_cross_regular_v2_dmrg_step\n","dmrg_rank_kick = 0\n","\n","# Test Set Generation Parameters\n","sizeOmega_C = 2000\n","TEST_SET_SEED = 610014611\n","\n","# Parameters for Post-TT-Cross processing\n","increase_amount = 0 # Integer that determines how many units the TT rank is increased\n","sizeOmegaExtra = 0 # Extra points to augment training set\n","\n","# Call the pipeline function to run the selected configuration\n","X_optimized_final, loss_hist_final, val_loss_hist_final = run_tensor_completion_pipeline(\n","    function_choice=function_to_approximate,\n","    optimization_method=optimization_method_choice,\n","    d_param=d,\n","    MODAL_SIZE=MODAL_SIZE,\n","    sizeOmega_max=sizeOmega_max,\n","    min_rank_to_try=min_rank_to_try,\n","    max_rank_to_try=max_rank_to_try,\n","    manual_rank=manual_rank,\n","    seed_tt_cross=seed_tt_cross,\n","    tol_flattening=tol_flattening,\n","    max_its_tt_cross=max_its_tt_cross,\n","    tol_precision=tol_precision,\n","    metric=metric,\n","    tt_cross_algorithm_func=tt_cross_algorithm_func,\n","    dmrg_rank_kick=dmrg_rank_kick,\n","    sizeOmega_C=sizeOmega_C,\n","    TEST_SET_SEED=TEST_SET_SEED,\n","    increase_amount=increase_amount,\n","    sizeOmegaExtra=sizeOmegaExtra,\n","    verbose=True,\n","    graphics=True\n",")\n","\n","# --- Final Results Summary ---\n","print(\"\\n--- Pipeline Execution Complete ---\")\n","if X_optimized_final is not None:\n","    # Assuming X_optimized_final is a t3f.TensorTrain object or similar\n","    print(f\"Final optimized TT approximation (first core shape): {X_optimized_final.cores[0].shape if hasattr(X_optimized_final, 'cores') and X_optimized_final.cores else 'N/A'}\")\n","if loss_hist_final is not None:\n","    print(f\"Final training loss: {loss_hist_final[-1]:.6f}\")\n","    print(f\"Final validation loss: {val_loss_hist_final[-1]:.6f}\")\n","else:\n","    print(\"No optimization method was chosen, so no final training/validation loss history from optimization.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTVki0IwhP2k","executionInfo":{"status":"ok","timestamp":1749557428522,"user_tz":-120,"elapsed":8671,"user":{"displayName":"CLEMENTE ESQUINAS COVES","userId":"18187455758607316569"}},"outputId":"7813aca5-7c32-4347-fbca-2f4951d6e2b3"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Running Tensor Completion Pipeline for Function: f2 ---\n","\n","--- Running TT-Cross for Function f2 ---\n","Iniciando barrido de rangos para TT-Cross (Métrica: Error Relativo L2)\n","Validación externa: 2000 puntos.\n","------------------------------\n","\n","Probando con rango TT objetivo: 1\n","  Rango TT inicial: (1, 1, 1, 1, 1, 1, 1, 1, 1)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (1, 1, 1, 1, 1, 1, 1, 1, 1)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 2000\n","  Pares (índice, valor) recolectados individualmente: 2000\n","  Iteraciones completadas: 20\n","  Razón de parada del algoritmo TT-Cross: max_iterations\n","  Tiempo de ejecución para este rango: 0.40 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 2.7283022481e-01\u001b[0m\n","  ¡Nueva mejor aproximación automática encontrada!\n","    Rango: 1, Error Relativo L2 (Validación): 2.7283022481e-01, Evaluaciones: 2000\n","\n","Probando con rango TT objetivo: 2\n","  Rango TT inicial: (2, 2, 2, 2, 2, 2, 2, 2, 2)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (2, 2, 2, 2, 2, 2, 2, 2, 2)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 7200\n","  Pares (índice, valor) recolectados individualmente: 7200\n","  Iteraciones completadas: 20\n","  Razón de parada del algoritmo TT-Cross: max_iterations\n","  Tiempo de ejecución para este rango: 1.12 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 1.3055947291e-01\u001b[0m\n","  ¡Nueva mejor aproximación automática encontrada!\n","    Rango: 2, Error Relativo L2 (Validación): 1.3055947291e-01, Evaluaciones: 7200\n","\n","Probando con rango TT objetivo: 3\n","  Rango TT inicial: (3, 3, 3, 3, 3, 3, 3, 3, 3)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (3, 3, 3, 3, 3, 3, 3, 3, 3)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 3900\n","  Pares (índice, valor) recolectados individualmente: 3900\n","  Iteraciones completadas: 5\n","  Razón de parada del algoritmo TT-Cross: flattening\n","  Tiempo de ejecución para este rango: 0.85 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 1.0168524509e-01\u001b[0m\n","  ¡Nueva mejor aproximación automática encontrada!\n","    Rango: 3, Error Relativo L2 (Validación): 1.0168524509e-01, Evaluaciones: 3900\n","\n","Probando con rango TT objetivo: 4\n","  Rango TT inicial: (4, 4, 4, 4, 4, 4, 4, 4, 4)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (4, 4, 4, 4, 4, 4, 4, 4, 4)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 9520\n","  Pares (índice, valor) recolectados individualmente: 9520\n","  Iteraciones completadas: 7\n","  Razón de parada del algoritmo TT-Cross: flattening\n","  Tiempo de ejecución para este rango: 1.06 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 3.4838106696e-02\u001b[0m\n","  ¡Nueva mejor aproximación automática encontrada!\n","    Rango: 4, Error Relativo L2 (Validación): 3.4838106696e-02, Evaluaciones: 9520\n","\n","Probando con rango TT objetivo: 5\n","  Rango TT inicial: (5, 5, 5, 5, 5, 5, 5, 5, 5)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (5, 5, 5, 5, 5, 5, 5, 5, 5)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 23100\n","  Pares (índice, valor) recolectados individualmente: 23100\n","  Iteraciones completadas: 11\n","  Razón de parada del algoritmo TT-Cross: flattening\n","  Tiempo de ejecución para este rango: 2.33 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 1.2500844665e-02\u001b[0m\n","\n","Probando con rango TT objetivo: 6\n","  Rango TT inicial: (6, 6, 6, 6, 6, 6, 6, 6, 6)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (6, 6, 6, 6, 6, 6, 6, 6, 6)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 9000\n","  Pares (índice, valor) recolectados individualmente: 9000\n","  Iteraciones completadas: 3\n","  Razón de parada del algoritmo TT-Cross: precision\n","  Tiempo de ejecución para este rango: 0.16 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 4.3640312592e-03\u001b[0m\n","  ¡Nueva mejor aproximación automática encontrada!\n","    Rango: 6, Error Relativo L2 (Validación): 4.3640312592e-03, Evaluaciones: 9000\n","\n","Probando con rango TT objetivo: 7\n","  Rango TT inicial: (7, 7, 7, 7, 7, 7, 7, 7, 7)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (7, 7, 7, 7, 7, 7, 7, 7, 7)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 8120\n","  Pares (índice, valor) recolectados individualmente: 8120\n","  Iteraciones completadas: 2\n","  Razón de parada del algoritmo TT-Cross: precision\n","  Tiempo de ejecución para este rango: 0.18 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 1.8216316310e-03\u001b[0m\n","  ¡Nueva mejor aproximación automática encontrada!\n","    Rango: 7, Error Relativo L2 (Validación): 1.8216316310e-03, Evaluaciones: 8120\n","\n","Probando con rango TT objetivo: 8\n","  Rango TT inicial: (8, 8, 8, 8, 8, 8, 8, 8, 8)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (8, 8, 8, 8, 8, 8, 8, 8, 8)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 15840\n","  Pares (índice, valor) recolectados individualmente: 15840\n","  Iteraciones completadas: 3\n","  Razón de parada del algoritmo TT-Cross: precision\n","  Tiempo de ejecución para este rango: 0.31 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 8.7384699821e-04\u001b[0m\n","\n","Probando con rango TT objetivo: 9\n","  Rango TT inicial: (9, 9, 9, 9, 9, 9, 9, 9, 9)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (9, 9, 9, 9, 9, 9, 9, 9, 9)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 13320\n","  Pares (índice, valor) recolectados individualmente: 13320\n","  Iteraciones completadas: 2\n","  Razón de parada del algoritmo TT-Cross: precision\n","  Tiempo de ejecución para este rango: 0.24 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 4.9325412619e-04\u001b[0m\n","  Guardando datos para el rango especificado manualmente: 9.\n","\n","Probando con rango TT objetivo: 10\n","  Rango TT inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Forma TTML inicial: (10, 10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Rango TT final de la aproximación: (10, 10, 10, 10, 10, 10, 10, 10, 10)\n","  Evaluaciones totales de vectores (llamadas a physical_point_index_fun): 16400\n","  Pares (índice, valor) recolectados individualmente: 16400\n","  Iteraciones completadas: 2\n","  Razón de parada del algoritmo TT-Cross: precision\n","  Tiempo de ejecución para este rango: 0.36 segundos\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C, recalculado): 1.3513247373e-04\u001b[0m\n","\n","------------------------------\n","Selección Final de la Aproximación TT\n","Priorizando resultados del rango especificado manualmente: 9.\n","  Advertencia: El rango manual 9 usó 13320 evaluaciones, excediendo sizeOmega (10000).\n","\n","Procesando los 13320 pares (índice, valor) recolectados para el TT elegido...\n","Conteo de elementos antes de eliminar repeticiones:\n","  best_indices_list: 13320\n","  best_evaluations_list: 13320\n","\n","Conteo de elementos después de eliminar repeticiones:\n","  best_indices_list: 11124\n","  best_evaluations_list: 11124\n","\n","El número de elementos coincide en ambas listas después de eliminar repeticiones.\n","¡Todos los elementos de final_evals corresponden a la evaluación de custom_function en los nodos de Chebyshev mapeados desde final_indices!\n","  Procesamiento completado. Obtenido conjunto de entrenamiento con 11124 puntos.\n","\n","------------------------------\n","Resumen de la Aproximación TT Elegida\n","Detalles de selección: Rango seleccionado manualmente: 9 (Nota: usó 13320 evaluaciones, límite sizeOmega=10000)\n","  Rango TT objetivo del barrido: 9\n","  Rangos TT reales de los cores: (9, 9, 9, 9, 9, 9, 9, 9, 9)\n","  \u001b[94mError Relativo L2 (en Conjunto de Entrenamiento Procesado): 8.4327781398e-05\u001b[0m\n","  \u001b[92mError Relativo L2 (en Conjunto de Validación Omega_C): 4.9325412619e-04\u001b[0m\n","  Evaluaciones totales de vectores (función original): 13320\n","  Pares (índice, valor) recolectados originalmente: 13320\n","    (Número de índices recolectados: 13320)\n","    (Tamaño del conjunto de entrenamiento procesado: 11124)\n","\n","  Primeros 5 pares (índice, valor) recolectados originalmente (ejemplo):\n","    0: Índice=0, 0, 1, 6, 9, 1, 7, 8, 3, 6, Evaluación=1.79169090\n","    1: Índice=0, 1, 3, 6, 9, 1, 7, 8, 3, 6, Evaluación=1.91552188\n","    2: Índice=0, 3, 8, 4, 1, 0, 9, 8, 3, 6, Evaluación=3.21121057\n","    3: Índice=0, 4, 8, 4, 1, 0, 9, 8, 3, 6, Evaluación=3.51833451\n","    4: Índice=0, 5, 1, 6, 9, 1, 7, 8, 3, 6, Evaluación=2.75352811\n","----------------------------------------\n","Fin del barrido de rangos TT-Cross.\n","----------------------------------------\n","\n","--- Checking Approximation Accuracy for Model Type: TensorTrain ---\n","\n","--- Results for Training Set (11124 points) ---\n","\n","Summary of Relative L2 Error (Norm) for Training Set:\n","\u001b[94mTotal Relative L2 Error (Norm): 8.4327781398e-05\u001b[0m\n","(Note: Relative L2 Error is a global norm metric; per-point error percentiles are not typically calculated for this value)\n","\n","--- Results for Test Set (2000 points) ---\n","\n","Summary of Relative L2 Error (Norm) for Test Set:\n","\u001b[92mTotal Relative L2 Error (Norm): 4.9325412619e-04\u001b[0m\n","(Note: Relative L2 Error is a global norm metric; per-point error percentiles are not typically calculated for this value)\n","TT-Cross final validation error for Function f2: 0.000493\n","Total vector evaluations after TT-Cross: 27524\n","\n","Optimization method set to None. Stopping after TT-Cross for Function f2.\n","\n","--- Pipeline Execution Complete ---\n","Final optimized TT approximation (first core shape): (1, 10, 9)\n","No optimization method was chosen, so no final training/validation loss history from optimization.\n"]}]}]}
